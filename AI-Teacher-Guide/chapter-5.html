<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>Using AI with words and sentences</title>
<link href="/ai/css/ai-teacher-guide.css" rel="stylesheet">
<link href="/ai/css/oer-style.css" rel="stylesheet">
<link rel="icon" type="image/png" href="/ai/images/eCraft2Learn-Favicon.png" />
<script src="/ai/js/ai-guide.js"></script>
</head>
<body>
<script src="/ai/js/translate.js"></script>
<h2>A guide to building AI apps and artefacts</h2>
<h3>Chapter 5 - Using AI with words and sentences</h3>
<h4>Ken Kahn, University of Oxford</h4>
<h3 id="browser-compatibility">Browser compatibility</h3>
<p>This chapter of the guide includes many interactive elements that currently run best in the Chrome browser.
See the <a href="troubleshooting.html" target="_blank">troubleshooting guide</a>
for how to deal with problems encountered.</p>
<h3>Introduction</h3>
<p>
AI programs can do many things with text.
These include
<ol>
<li>
Answering questions (including more intelligent handling of web searches).
</li>
<li>
Summarising text.
</li>
<li>
Detecting the sentiment of the text (positive or negative? happy or sad? angry?).
</li>
<li>
Authoring text (many sports and financial
<a href="https://www.bbc.com/news/technology-342040522" target="_blank">news articles are written by computers</a> today).
</li>
<li>
Determining the grammatical structure of a sentence.
</li>
<li>
Translation between languages.
</li>
</ol>
</p>

<h4>Benefits and risks</h4>
<p>
to do
</p>

<h4>Doing arithmetic with words and sentences</h4>
<p>
While computers can deal with text as strings of characters a technique called
<a href="https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469" target="_blank">word embedding</a>
works by converting words into a long list of numbers.
These numbers can either be created by humans where a number has a meaning such as 'minimum size', 'maximum size' or 'average life expectancy'.
Most AI programs instead use numbers created by machine learning (see <a href="chapter4.html">the previous chapter</a>).
The numbers are created by considering text with billions word (e.g. all Wikipedia pages in a given language).
People don't understand what the numbers mean but similar words have similar numbers and unrelated words have very different numbers.
Each number measures a 'feature' of the word but what feature is a mystery.
</p>
<h4>Turning a word into lots of numbers</h4>
<p>
We have created Snap! blocks for exploring how word embeddings can be used to find similar words, 
words which are between other words, and most surprisingly solve word analogy problems.
The <span class='block-name'>features of</span> block will report a list of 300 numbers.
You can think of the numbers as placing the word in a 300-dimensional space.
The numbers were adjusted so all 10,000 words fit inside a 300-dimensional 
<a href="https://en.wikipedia.org/wiki/Hypersphere" target="_blank">hypersphere<a/>.
<figure class = "snap-iframe"
        id = "features of"
        container_style = "width: 500px; height: 600px" 
        caption = "A block for converting words into lists of 300 numbers. TRY IT! (First time it may take some time to load.)">
</figure>
</p>
<p>
<h4>Finding the closest word to a list of feature numbers</h4>
<p>
A program can search through all the words to find the word that is closest to a list of numbers.
The <span class="block-name">closest word to</span> reporter block does this.
There are two common ways of measuring distance in a high-dimensional space.
One is <a href="https://en.wikipedia.org/wiki/Euclidean_distance" target="_blank">Euclidean distance</a>
which is a generalisation of how distance is computed in 2 and 3 dimensional space.
The idea is to take the sum of the squares of the differences along each of the 300 dimensions and then report the square root of that.
The other measure is called <a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank">cosine similarity</a>.
Both work pretty well in the <span class="block-name">closest word to</span> reporter which lets you choose which to use.
<figure class = "snap-iframe"
        id = "closest word to"
        container_style = "width: 800px; height: 300px" 
        caption = "Finding the word closest to a word. TRY IT!">
</figure>
</p>

<h4>Finding the word half way between two other words</h4>
<p>
You can take two words and average their features by adding together corresponding numbers and dividing the result by 2.
You can then use the <span class="block-name">closest word to</span> reporter to find the word closest to the average.
<figure class = "snap-iframe"
        id = "word average"
        container_style = "width: 800px; height: 350px" 
        caption = "Finding the word closest to the average of two words. TRY IT!">
</figure>
Try averaging more than two words.
And see what word is the closest to somewhere between two words other than the halfway point.
</p>

<h4>Using word embeddings to solve word analogy problems</h4>
<p>
One of the most surprising thing about word embeddings is that with the right formula one solve word analogy problems.
For example "man is to woman as father is to what?" can be expressed as "father-(man-woman)=x".
<figure class = "snap-iframe"
        id = "word analogy"
        container_style = "width: 850px; height: 300px" 
        caption = "Solving word analogy problems. TRY IT!">
</figure>
This works for grammatical analogies as well.
Try solving "slow is to slower as fast is to what?".
You might need to add 'fast' as an exception.
</p>

<h4>How does this work?</h4>
<p>
While we don't really know what the numbers mean they must be encoding lots of things about words such as
gender, grammatical category, family relationships, and hundreds more things.
But the numbers aren't perfect.
See if you can create some examples where the results are not good.
One knowwn problem with how the numbers are generated is that it combines features of differnt senses of the same word.
There is only one entry, for example, for 'jaguar' which combines the ways that word is used in sentences about cats and those about cars.
</p>

<h4 id="project-ideas">Possible project ideas using image recognition</h4>
<p>
<ol>
<li>
Try using word embeddings to explore the similarity of sentences.
One idea is to average all the words in the sentence.
This is called the <a href="https://en.wikipedia.org/wiki/Bag-of-words_model" target="_blank">bag of words</a>
technique since it ignores the order of the words just as if they were put in a bag.
</li>
</ol>
</p>

<h4>Future directions for this chapter</h4>
<p>
to do
</p>

<h3 id="additional-resources">Additional resources</h3>
<p>
to do
</p>
<p>
Return to 
<a class="guide-link" href="chapter-4.html">the previous chapter on machine learning</a>.
</p>
<script src="/ai/js/bottom-of-page.js"></script>
</body>
</html>