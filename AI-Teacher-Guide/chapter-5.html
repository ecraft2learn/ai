<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>Using AI with words and sentences</title>
<link href="/ai/css/ai-teacher-guide.css" rel="stylesheet">
<link href="/ai/css/oer-style.css" rel="stylesheet">
<link rel="icon" type="image/png" href="/ai/images/eCraft2Learn-Favicon.png" />
<script src="/ai/js/ai-guide.js"></script>
</head>
<body>
<script src="/ai/js/translate.js"></script>
<h2>A guide to building AI apps and artefacts</h2>
<h3>Chapter 5 - Using AI with words and sentences</h3>
<h4>Ken Kahn, University of Oxford</h4>
<h3 id="browser-compatibility">Browser compatibility</h3>
<p>This chapter of the guide includes many interactive elements that currently run best in the Chrome browser.
See the <a href="troubleshooting.html" target="_blank">troubleshooting guide</a>
for how to deal with problems encountered.</p>
<h3>Introduction</h3>
<p>
AI programs can do many things with text.
These include
<ol>
<li>
Answering questions (including more intelligent handling of web searches).
</li>
<li>
Summarising text.
</li>
<li>
Detecting the sentiment of the text (positive or negative? happy or sad? angry?).
</li>
<li>
Authoring text (many sports and financial
<a href="https://www.bbc.com/news/technology-342040522" target="_blank">news articles are written by computers</a> today).
</li>
<li>
Determining the grammatical structure of a sentence.
</li>
<li>
Translation between languages.
</li>
</ol>
</p>

<h4>Doing arithmetic with words and sentences</h4>
<p>
While computers can deal with text as strings of characters a technique called
<a href="https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469" target="_blank">word embedding</a>
works by converting words into a long list of numbers.
These numbers can either be created by humans where a number has a meaning such as 'minimum size', 'maximum size' or 'average life expectancy'.
Most AI programs instead use numbers created by machine learning (see <a href="chapter4.html">the previous chapter</a>).
The numbers are created by considering text with billions word (e.g. all Wikipedia pages in a given language).
People don't understand what the numbers mean but similar words have similar numbers and unrelated words have very different numbers.
Each number measures a 'feature' of the word but what feature is a mystery.
</p>
<p>
The word embeddings used in this chapter were created by <a href="https://fasttext.cc/docs/en/support.html" target="_blank">Facebook</a>.
They trained their machine learning models on 157 different languages on all Wikipedia articles in each language.
Even though that was about a billion words each that wasn't enough,
so they also trained their models on tens of billions more words found by <a href="http://commoncrawl.org/" target="_blank">crawling the web</a>.
They created tables for each language of at least a million words.
The blocks described here provide the 20,000 most common words for 13 languages.
(Larger tables and more languages can be added. Send requests to toontalk@gmail.com.)
</p>
<h4>Turning a word into lots of numbers</h4>
<p>
We have created Snap! blocks for exploring how word embeddings can be used to find similar words, 
words which are between other words, and most surprisingly solve word analogy problems.
The <span class='block-name'>features of</span> block will report a list of 300 numbers.
If the language field is left empty the default language will be used.
You can think of the numbers as placing the word in a 300-dimensional space.
The numbers were adjusted so all 20,000 words fit inside a 300-dimensional 
<a href="https://en.wikipedia.org/wiki/Hypersphere" target="_blank">hypersphere</a> with a radius of 1.
There are databases with word embeddings for one million words
but loading and searching such a large data set would be very slow.
The <span class='block-name'>features of</span> block is based upon the most frequently occurring 20,000 entries that 
are lower case and contain only letters (no punctuation or digits).
</p>
<figure class = "snap-iframe"
        id = "features of"
        container_style = "width: 800px; height: 600px" 
        caption = "A block for converting words into lists of 300 numbers. TRY IT! (First time it may take some time to load.)">
</figure>
<p>
<h4>Finding the closest word to a list of feature numbers</h4>
<p>
A program can search through all the words to find the word that is closest to a list of numbers.
The <span class="block-name">closest word to</span> reporter block does this.
</p>

<p class="student-guide advanced-topic">Click to read an advanced topic</p>
<span class="advanced-topic-body">
<h4>Different ways of measuring distances in high dimensional spaces</h4>
There are two common ways of measuring distance in a high-dimensional space.
One is <a href="https://en.wikipedia.org/wiki/Euclidean_distance" target="_blank">Euclidean distance</a>
which is a generalisation of how distance is computed in 2 and 3 dimensional space.
The idea is to take the sum of the squares of the differences along each of the 300 dimensions and then report the square root of that.
The other measure is called <a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank">cosine similarity</a>.
Both work pretty well in the <span class="block-name">closest word to</span> reporter which lets you choose which to use.
While they usually agree sometimes small differences can be observed.
For example, the third closest word to "dog" can be "canine" or "puppy" depending on the measure.
</span>

<figure class = "snap-iframe"
        id = "closest word to"
        container_style = "width: 800px; height: 600px" 
        caption = "Finding the word closest to a word. TRY IT!">
</figure>
</p>

<h4>Finding the word half way between two other words</h4>
<p>
You can take two words and average their features by adding together corresponding numbers and dividing the result by 2.
You can then use the <span class="block-name">closest word to</span> reporter to find the word closest to the average.
<figure class = "snap-iframe"
        id = "word average"
        container_style = "width: 750px; height: 450px" 
        caption = "Finding the word closest to the average of two words. TRY IT!">
</figure>
Try averaging more than two words.
And see what word is the closest to somewhere between two words other than the halfway point.
</p>

<h4>Using word embeddings to solve word analogy problems</h4>
<p>
One of the most surprising thing about word embeddings is that with the right formula one solve word analogy problems.
For example "man is to woman as father is to what?" can be expressed as "father-(man-woman)=x".
<figure class = "snap-iframe"
        id = "word analogy"
        container_style = "width: 850px; height: 450px" 
        caption = "Solving word analogy problems. TRY IT! Then find other analogies that work.">
</figure>
This works for grammatical analogies as well.
Try solving "slow is to slower as fast is to what?".
You might need to add 'fast' as an exception.
</p>

<h4>Mapping 300 dimensional points to two-dimensional points</h4>
<p>
No one can visualise 300-dimensional space.
There are techniques for giving an impression of the relationships between very high-dimensional points
by mapping the points to two or three dimensions.
We use a technique called
<a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" target="_blank">t-SNE</a>.
It can be understood as a physics simulation where all points in crowded areas repel each other and
all points are attracted to those with a small distance away (in high-dimensional space).
This <a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/en/projector.json" target="_blank">
data projector</a> displays all 20,000 English words in two or three dimensions using either t-SNE
or PCA (<a href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank">principal component analysis</a>).
You can also use the projector to see the word embeddings of these languages:
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/de/projector.json" target="_blank">German</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/el/projector.json" target="_blank">Greek</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/es/projector.json" target="_blank">Spanish</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/fr/projector.json" target="_blank">French</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/fi/projector.json" target="_blank">Finnish</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector.json" target="_blank">Hindi</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/id/projector.json" target="_blank">Indonesian</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/it/projector.json" target="_blank">Italian</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/ja/projector.json" target="_blank">Japanese</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/pt/projector.json" target="_blank">Portuguese</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/si/projector.json" target="_blank">Sinhalese</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/sv/projector.json" target="_blank">Swedish</a>, and
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/zh/projector.json" target="_blank">Chinese</a>.

Note that it takes several hundred iterations of t-SNE before it settles down on a good mapping from 300 dimensions.
You can also search for words and their neighbours and create bookmarks.
The above links launch the projector with a bookmark showing t-SNE and highlighting the hundred words closes to 'dog'.
<figure>
<img src="images/projector-bookmarks.png" class="center">
<figcaption>In the lower right corner you can select projector bookmarks</figcaption></figure>
</p>
<p>
Here is a program that displays 50 random words at the location generated by t-SNE.
<figure class = "snap-iframe"
        id = "random word locations"
        full_screen = "true"
        container_style = "width: 1000px; height: 800px" 
        caption = "Repeatedly picks a word and displays it at its t-SNE location. TRY IT!">
</figure>
</p>

<h4>Word embeddings can do translations</h4>
<p>
What would happen if you took, for example, the features of the English word 'dog' and 
asked for the closest word in say French.
Try this with different source and target languages and different words.
Compare it with <a href="https://translate.google.com/" target="_blank">Google Translate</a>.
Tip: it is easy to copy and paste words that your keyboard can't type from the Google Translate page.
Supported languages are Chinese, English, Finnish, French, German, Greek, Hindi, Indonesian, Italian,
Japanese, Lithuanian, Portuguese, Sinhalese, Spanish, and Swedish.

<figure class = "snap-iframe"
        id = "translate exercise"
        container_style = "width: 800px; height: 400px" 
        caption = "Find the closest word in a different language.">
</figure>
</p>
<h4>A 'Guess My Word' game using word embeddings</h4>
<p>
The following game picks a random word and gives the player warmer or colder feedback as the player makes guesses.
This does so by comparing the distance to the secret word with the previous distance.
It uses the <span class="command-block">location of ...</span> reporter block to display your guesses.
The game is very hard!
There are many ways to make the game better.
See if you can!
<figure class = "snap-iframe"
        id = "guess my word"
        full_screen = "true"
        container_style = "width: 800px; height: 800px" 
        caption = "Click the green flag to play Guess My Word. TRY IT!">
</figure>
</p>
<h4>Benefits and risks using word embeddings</h4>
<p>
Word embeddings can be used as a component in AI programs that do
sentiment analysis, entity detection, recommendations, summarising text, translation, and question answering.
This is typically done by replacing the words in a text with their embeddings and then doing machine learning on the 
approximate meaning of the words.
This makes the systems work better with synonyms and paraphrasings.
</p>
<p>
Word embeddings are learned by examining text with billions of words.
These texts may have captured
<a href="https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html" target="_blank">
societal biases</a>.
For example, the following example seems to have captured the bias that butchers are male and bakers female.
Other word embedding databases have the bias that doctors are male and nurses female.
<figure class = "snap-iframe"
        id = "word analogy bias"
        container_style = "width: 850px; height: 450px" 
        caption = "Bias in word analogy solutions. See if you can find more.">
</figure>
</p>

<h4>How does this work?</h4>
<p>
While we don't really know what the numbers mean they must be encoding lots of things about words such as
gender, grammatical category, family relationships, and hundreds more things.
But the numbers aren't perfect.
See if you can create some examples where the results are not good.
One known problem with how the numbers are generated is that it combines features of different senses of the same word.
There is only one entry, for example, for 'bank' which combines the ways that word is
used in sentences about financial institutions and those about the sides of rivers.
This can cause words to be closer than they should be.
For example, "rat" and "screen" end up being closer together than otherwise due to "rat" being close to "mouse"
and "mouse" (the computer input device) being close to (computer) "screen".
This is a problem researchers are working on.
Another problem is that sometimes short phrases act like words.
"Ice cream" for example has no word embedding while "sherbet" and "sorbet" do.
</p>
<h4>A sample project using word embeddings for translation</h4>
<p>
Here is
<a href="/ai/snap/snap.html?project=translate" target="_blank">a program that asks the user for two languages</a>,
obtains the feature vector of a random word from the first language,
and then displays several of the words closest to that feature vector.
It places the words in the t-SNE two-dimensional approximation of where the 300-dimensional words really are.
</p>
<h4>How does translation using word embeddings work?</h4>
<p>
The word embeddings for each language were generated independently based upon text from Wikipedia and the web.
The location in 300-dimensional space of the features for a word like 'dog' have no relationship 
to the location of features of translations of the word 'dog'.
<a href="https://arxiv.org/abs/1309.4168" target="_blank">Researchers</a>
noticed that in most (all?) languages some words are close together.
For example 'dog', 'dogs', puppy', and 'canine' are close.
Words like 'wolf', 'cow', and 'mouse' are not as close to the words near 'dog'.
And all of these words are far from abstract words like 'truth' and 'logic'.
So they discovered that it is possible to find a rotation that will cause many word embeddings
in one language to cause translations of those words to be close.
The way it was done at first and in these Snap! blocks is by giving a program a word list between
English and each other language.
500 or 1000 words is enough to find a good rotation that brings most of the other 19000 words
close to where their translations are.
While it is impressive that translation works at all given a word list that covers 5% of the vocabulary
<a href="https://arxiv.org/abs/1710.04087" target="_blank">Word Translation Without Parallel Data</a>
describes a technique that uses no word lists or translated texts.
A rotation is all that is needed because all the word embeddings are centred around zero
so they don't need to be translated (in the mathematical sense, i.e. moved) as well.
But note that the translation happens in 299 dimensions!
</p>

<h4 id="project-ideas">Possible project ideas using image recognition</h4>
<p>
<ol>
<li>
Try using word embeddings to explore the similarity of sentences.
One idea is to average all the words in the sentence.
This is called the <a href="https://en.wikipedia.org/wiki/Bag-of-words_model" target="_blank">bag of words</a>
technique since it ignores the order of the words just as if they were put in a bag.
</li>
<li>
Find a chain of similar words by finding the nearest word to the starting word.
Then repeatedly find the nearest word to that while making sure to never repeat the same word.
Use this to repeatedly change random words one at a time in a famous poem or text
(e.g. "roses are red and violets are blue").
</li>
<li>
Make word games using word embeddings.
For example, something like <a href="https://experiments.withgoogle.com/semantris" target="_blank">Semantris</a>,
a semantic version of Tetris.
A bilingual version Semantris might be a good idea.
</li>
<li>
Be creative! Word embeddings are new and there is much that remains to discover.
</li>
</ol>
</p>

<h4>Future directions for this chapter</h4>
<p>
There is plenty more that AI programs can do with language including
determining the grammatical structure of sentences (this is called "parsing"),
figuring out the sentiment in some text,
and question answering.
We are considering integrating some of the tools that have been developed
for visualising thousands of words inside of a high dimensional space.
More to come here.
</p>
<p>
Currently the word embeddings blocks excludes all proper nouns.
If we added them one could solve analogies such as Paris is to France as Berlin is to X.
</p>

<h3 id="additional-resources">Additional resources</h3>
<p>
Wikipedia's <a href="https://en.wikipedia.org/wiki/Word_embedding" target="_blank">word embedding</a> article is short and
written for an advanced audience.
The Facebook team wrote a paper detailing how they generated the word embeddings we used here:
E. Grave, P. Bojanowski, P. Gupta, A. Joulin, T. Mikolov,
<i><a href="https://arxiv.org/abs/1802.06893" target="_blank">Learning Word Vectors for 157 Languages</a></i>.
The <a href="https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469" target="_blank">Understanding word vectors</a> web page
has a very good introduction to the subject and the contains examples that are helpful but require familiarity with Python.
This <a href="https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html" target="_blank">
Google blog about biases in word embeddings</a> is very good and clear.
<a href="https://distill.pub/2016/misread-tsne/" target="_blank">How to Use t-SNE Effectively</a>
is a clear interactive description of how t-SNE works.
<a href="https://arxiv.org/abs/1309.4168" target="_blank">Exploiting Similarities among Languages for Machine Translation</a>
pioneered the idea adjusting the word embeddings to support translation.
<a href="https://arxiv.org/abs/1710.04087" target="_blank">Word Translation Without Parallel Data</a>
explores how word embeddings can be used for translation without using word lists or translated texts.
<a href="https://arxiv.org/abs/1710.04087" target="_blank">Word Translation Without Parallel Data</a>
<a href="https://projector.tensorflow.org/" target="_blank">projector.tensorflow.org</a>
is a great website for interactively exploring different ways of visualising high-dimensional spaces.
Here is a video of <a href="https://www.youtube.com/watch?v=RJVL80Gg3lA" target="_blank">
a nice talk by Laurens van der Maaten</a> who invented the idea of t-SNE.
</p>
<p>
Return to 
<a class="guide-link" href="chapter-4.html">the previous chapter on machine learning</a>.
</p>
<script src="/ai/js/bottom-of-page.js"></script>
</body>
</html>