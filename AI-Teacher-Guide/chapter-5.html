<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>Using AI with words and sentences</title>
<link href="../css/ai-teacher-guide.css" rel="stylesheet">
<link rel="icon" type="image/png" href="../images/eCraft2Learn-Favicon.png" />
<script src="../js/ai-guide.js"></script>
</head>
<body>
<script src="../js/translate.js"></script>
<h2>A guide to building AI apps and artifacts</h2>
<h3>Chapter 5 - Using AI with words and sentences</h3>
<h4>Ken Kahn, University of Oxford</h4>
<p class="resources">
You can import the blocks presented here as a
<a href="/ai/snap/snap.html?project=words&editMode" target="_blank">project</a> or 
download them as a <a href="words blocks.xml" download target="_blank">library</a> to import into your projects.
</p>
<div id="table-of-contents"></div>
<h4 class="guide-to-guide-white" id="browser-compatibility">Browser compatibility</h3>
<p class="guide-to-guide">
This chapter of the guide includes many interactive elements that are known to run well in the Chrome or Edge browser.
See the <a href="troubleshooting.html" target="_blank">troubleshooting guide</a>
for how to deal with problems encountered.
</p>
<h4 class="background-information-white" id="introduction">Introduction</h3>
<p class="background-information">
AI programs can do many things with text.
These include
</p>
<ol class="background-information">
<li>
Answering questions (including more intelligent handling of web searches).
</li>
<li>
summarizing text.
</li>
<li>
Detecting the sentiment of the text (positive or negative? happy or sad? angry?).
</li>
<li>
Authoring text (many sports and financial
<a href="https://www.bbc.co.uk/news/technology-34204052" target="_blank">news articles are written by computers</a> today).
</li>
<li>
Determining the grammatical structure of a sentence.
</li>
<li>
Translating between languages.
</li>
</ol>

<h4 class="background-information-white" id="arithmetic-with-words">Doing arithmetic with words and sentences</h4>
<p class="background-information">
While computers can deal with text as strings of characters, a technique called
<a href="https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469" target="_blank">word embedding</a>
works by converting words into a long list of numbers.
These numbers can either be created by humans where each number has a meaning such as 'minimum size', 'maximum size' or 'average life expectancy'.
Most AI programs instead use numbers created by machine learning
(see <a href="chapter4.html">the previous chapter</a> and <a href="chapter6.html">the next chapter</a>).
The numbers are created by training machine learning models on billions of words (e.g. all Wikipedia pages in a given language).
People don't understand what the numbers mean but similar words have similar numbers and
unrelated words have very different numbers.
Each number measures a 'feature' of the word but what that feature is is a mystery.
</p>
<p class="background-information">
The word embeddings used in this chapter were created by
<a href="https://fasttext.cc/docs/en/support.html" target="_blank">researchers at Facebook</a>.
They trained their machine learning models on 157 different languages on all Wikipedia articles in each language.
Even though that was about a billion words each that wasn't enough,
so they also trained their models on tens of billions more words found by <a href="http://commoncrawl.org/" target="_blank">crawling the web</a>.
They created tables for each language of at least a million different words.
The blocks described here provide the 20,000 most common words for 15 languages
(Chinese, English, Finnish, French, German, Greek, Hindi, Indonesian, Italian,
Japanese, Lithuanian, Portuguese, Sinhalese, Spanish, and Swedish).
(Larger tables and more languages can be added. Send requests to toontalk@gmail.com.)
</p>
<h4 class="instructions-white" id="words-to-numbers">Turning a word into lots of numbers</h4>
<p class="instructions">
We have created Snap! blocks for exploring how word embeddings can be used to find similar words, 
words which are between other words, and most surprisingly solve word analogy problems.
The <span class='block-name'>features of word</span> block will report a list of 300 numbers.
If the language field is left empty, the default language will be used.
You can think of the numbers as placing the word in a 300-dimensional space.
The numbers were adjusted so all 20,000 words fit inside a 300-dimensional 
<a href="https://en.wikipedia.org/wiki/Hypersphere" target="_blank">hypersphere</a> with a radius of 1.
There are databases with word embeddings for one million words
but loading and searching such a large data set would be very slow.
The <span class='block-name'>features of word</span> block is based upon the most frequently occurring 20,000 entries that 
are lower case (no proper nouns) and contain only letters (no punctuation or digits).
</p>
<figure class = "snap-iframe"
        id = "features of"
        container_style = "width: 550px; height: 600px" 
        caption = "A block for converting words into lists of 300 numbers. TRY IT! (First time it may take some time to load.)">
</figure>

<h4 class="instructions-white" id="closest-word">Finding the closest word to a list of feature numbers</h4>
<p class="instructions">
A program can search through all the words to find the word that is closest to a list of numbers.
The <span class="block-name">closest word to</span> reporter block does this.
</p>

<p class="advanced-topic">Click to read an advanced topic</p>
<span class="advanced-topic-body">
<h4 class="advanced-information-white" id="measuring-distance">Different ways of measuring distances in high dimensional spaces</h4>
<p class="background-information advanced-information">
There are two common ways of measuring distance in a high-dimensional space.
One is <a href="https://en.wikipedia.org/wiki/Euclidean_distance" target="_blank">Euclidean distance</a>
which is a generalisation of how distance is computed in 2 and 3 dimensional space.
The idea is to take the sum of the squares of the differences along each of the 300 dimensions and then report the square root of that.
The other measure is called <a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank">cosine similarity</a>.
Both work pretty well in the <i>full-featured version</i> of the <span class="block-name">closest word to</span> reporter
that lets you choose which to use.
While they usually agree sometimes small differences can be observed.
For example, the third closest word to "dog" can be "canine" or "puppy" depending on the measure.
The default is cosine similarity because researchers have found it works a little better in high-dimensional spaces.
Using the <span class="block-name">acos of</span> block on the <span class="block-name">cosine similarity</span> reporter
produces the angle between the inputs which is easier to understand.
</p>
</span>
</h4>

<figure class = "snap-iframe"
        id = "closest word to"
        container_style = "width: 800px; height: 500px" 
        caption = "Finding the word closest to a word. TRY IT! Then try it again after changing it to a language you know.">
</figure>
</p>

<h4 class="instructions-white" id="averaging-words">Finding the word half way between two other words</h4>
<p class="instructions">
You can take two words and average their features by adding together corresponding numbers and dividing the result by 2.
You can then use the <span class="block-name">closest word to</span> reporter to find the word closest to the average.
</p>
<figure class = "snap-iframe"
        id = "word average"
        container_style = "width: 750px; height: 300px" 
        caption = "Finding the word closest to the average of two words. TRY IT!">
</figure>
<p class="instructions">
Try averaging more than two words.
And see what word is the closest to somewhere between two words other than the halfway point.
</p>

<h4 class="instructions-white" id="word-analogy-problems">Using word embeddings to solve word analogy problems</h4>
<p class="instructions">
One of the most surprising things about word embeddings is that with the right formula
one can solve word analogy problems.
For example, "man is to woman as king is to what?" can be expressed as "king+(woman-man)=x".
</p>
<figure class = "snap-iframe"
        id = "word analogy"
        container_style = "width: 850px; height: 550px" 
        caption = "Solving word analogy problems. TRY IT! Then find other analogies that work.">
</figure>
<p class="instructions">
Note that "king is to man as woman is to what?" can be expressed as "woman+(king-man)=x".
And "king+(woman-man)=x" and "woman+(king-man)=x" are equivalent yet they solve different word analogy problems!
This use of word embeddings works for grammatical analogies as well.
Try solving "slow is to slower as fast is to what?".
You might need to add 'fast' as an exception.
</p>
<p class="background-information">
We have published some papers on using word embeddings. See <a href="/ai/index.html#publications">our publications</a>.
</p>
<p class="instructions">
Note that for word analogies of the form A is to B as C is to D that the distance between A-B to C-D is much
less than if A, B, C, and D were just words chosen at random.
One could search through a list of words by choosing four words and measuring the distance between
differences between pairs of words.
If one only keeps those where the distance is small one may discover new word analogy puzzles.
However, if one did this by searching through all 20,000 words then 3,998,800,199,970,000 combinations would
have to be considered.
Here is a <a href="../snap/snap.html?project=word analogy search&editMode&noRun" target="_blank">
project that enables one to search through a smaller set</a>.
</p>
<h4 class="instructions-white" id="closest-words">Finding all the 'closest' words</h4>
<p class="instructions">
If you were to use the <span class="block-name">closest word to</span> reporter to sort all
the words by distance to a list of features, it would take about a full day since it will have
to call the reporter 20,000 times.
Instead we provide the <span class="block-name">closest words to</span> reporter (notice the plural) that 
does it all at once in less than a second if the device has a
<a href="http://www.nvidia.com/object/what-is-gpu-computing.html" target="_blank">GPU</a>
(except the first time it is called it may take several seconds).
Optionally it can also report the distances as cosines.
</p>
<figure class = "snap-iframe"
        id = "sort closest words"
        container_style = "width: 900px; height: 500px" 
        caption = "Find all the words closest to a word or list of features. TRY IT!">
</figure>
<p class="instructions">
While one rarely needs all 20,000 words it might be interesting to compare two words by
seeing how many of the nearest 100 or 500 words of each are in common.
Think up other uses for this reporter.
</p>
<h4 class="instructions-white" id="closest-features">Finding how close some features are to a list of other features</h4>
<p class="instructions">
The <span class="block-name">features closest to list of features</span> block reports which features are closest to a given feature list.
It reports a list of all the listed features sorted by their distance to the given features.
The cosine similarity is also reported.
This block works well with word, sentence, or image embeddings.
Or a data table as illustrated below.
Note that the values should be in similar ranges.
(Weights are given as hundreds of kilograms for this reason.)
</p>
<figure class = "snap-iframe"
        id = "sort closest features"
        container_style = "width: 800px; height: 500px" 
        caption = "Find all the features closest to a feature list. TRY IT!">
</figure>
<h4 class="instructions-white" id="sentence-embeddings">Sentence embeddings</h4>
<p class="background-information">
One can compute sentence embeddings by averaging all the words in a sentence.
There is, however, a much better way that uses models that were trained to address
sentence-level tasks. 
The <a href="https://research.google/pubs/pub46808/" target="_blank">Universal Sentence Encoder</a> is one such model that
is available in the browser.
The reporters <span class="block-name">get features of sentence</span> and 
<span class="block-name">get features of sentences</span> use this the Universal Sentence Encoder
to produce 512 numbers for any sentence. (Really any sequence of words, it needn't be a grammatical sentence.)
</p>
<p class="background-information">
It can be used to measure how similar two sentences are.
Consider
<br>
A. How are you?
<br>
B. How old are you?
<br>
C. What is your age?
<br>
A and B have many words in common but it is B and C that have similar meanings.
</p>
<p class="instructions">
The <span class="block-name">get sentence features</span> block accepts multi-word texts and
reports a list of 512 numbers.
To compare sentences we use 
<a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank">cosine similarity</a>
that usually works better than Euclidean distance.
(The JavaScript version is used because it is faster.)
Using <a href="https://en.wikipedia.org/wiki/Inverse_trigonometric_functions" target="_blank">arc cosine</a>
converts the cosine similarity to an angle for clarity.
</p>
<figure class = "snap-iframe"
        id = "sentence embedding"
        container_style = "width: 900px; height: 500px"
        caption = "Here's a way to compare sentences. TRY IT!">
</figure>
<p class="background-information" id="confidence-guesser">
Sentence encodings can be used for more than comparing sentences.
Combined with deep learning models (see the <a href="chapter-6.html"> chapter on creating, training, and using machine learning models</a>)
they can be used to train a system to detect sentiment, emotions, topics, and more.
</p>
<figure class = "snap-iframe"
        id = "confidence guesser"
        full_screen = "true"
        container_style = "width: 900px; height: 500px"
        caption = "Here's a machine learning model that guesses your level of confidence. TRY IT!">
</figure>
<p class="instructions">
A full screen version of this program can be
<a href="../snap/snap.html?project=confidence guesser&editMode&noRun" target="_blank">found here</a>.
The machine learning model used was trained in this
<a href="chapter-6.html#confidences">project</a>.
</p>

<h4 class="instructions-white" id="manual-and-guide-ai-search">AI-aided search in the Snap! manual and the AI guide</h4>
<p class="background-information">
One use of sentence embeddings is information retrieval.
Consider the task of searching the Snap! manual or this AI programming guide.
String matching cannot take into account synonyms, different ways of saying the same thing,
or different spelling conventions.
In <a href="../snap/snap.html?project=QA guide&editMode&noRun" target="_blank">this sample search project</a>
sentence embeddings are used to compare the user's query with sentence fragments from the manual and guide.
By relying upon the
<a href="#closest-features"><span class="block-name">features closest to list of features</span> block</a>
the closest fragments are found very quickly.
The embeddings of all the fragments have been pre-computed so only the embedding of the user's query is needed.
Once the closest fragments have been computed we can fall back upon string search
since the fragments were derived from the document being searched.
</p>
<p class="instructions">
One can use this AI-augmented search while working with Snap! by downloading and then importing either
<a href="/ai/projects/Guide search.xml" download>AI programming guide search</a> or
<a href="/ai/projects/Manual search.xml" download>Snap! manual search</a>.
They work similarly and have similar interfaces.
The results tend to be better if the query is a grammatical phrase and not a question.
</p>
<p class="instructions">
The programming guide supports these keyboard commands:
</p>
<ul>
<li>
<b>g</b> - to enter a search query to the <b>g</b>uide
</li>
<li><b>t</b> - to query by <b>t</b>alking
</li>
<li><b>x</b> - to stop listening for queries
</li>
</ul>
<p class="instructions">
The Snap! manual supports these keyboard commands:
</p>
<ul>
<li>
<b>m</b> - to enter a search query to the <b>m</b>anual
</li>
<li>
<b>s</b> - to <b>s</b>peak a query
</li>
<li>
<b>x</b> - to stop listening for queries
</li>
</ul> 
<p class="instructions">
Both the manual and guide search can be imported into the same project.
But don't have both listening to your speech at the same time.
When you no longer need this assistance just delete the imported sprite.
</p>
<h4 class="instructions-white" id="drawing-word-embeddings">Drawing word embeddings</h4>
<p class="instructions">
It would be nice to visualize the 300 numbers associated with a word.
One way is to draw a succession of vertical lines, one for each feature.
</p>
<figure class = "snap-iframe"
        id = "draw word embedding"
        full_screen = "true"
        container_style = "width: 1000px; height: 500px"
        caption = "Here's a way to see the features of a word. TRY IT!">
</figure>
<p class="instructions">
A full screen version of this program can be
<a href="../snap/snap.html?project=draw%20word%20embedding&editMode&noRun" target="_blank">found here</a>.
See if you can create other ways of visualizing embeddings.
For example, to compare two embeddings perhaps the visualizations should be interleaved.
</p>
<h4 class="background-information-white" id="to-2d-points">Mapping 300 dimensional points to two-dimensional points</h4>
<p class="background-information">
No one can visualize 300-dimensional space.
There are techniques for giving an impression of the relationships between very high-dimensional points
by mapping the points to two or three dimensions.
We use a technique called
<a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" target="_blank">t-SNE</a>.
It can be understood as a physics simulation where all points in crowded areas repel each other and
all points are attracted to those with a small distance away (in high-dimensional space).
This <a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/en/projector.json" target="_blank">
data projector</a> displays all 20,000 English words in two or three dimensions using either t-SNE,
PCA (<a href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank">principal component analysis</a>),
or <a href="https://umap-learn.readthedocs.io/en/latest/how_umap_works.html">UMAP</a>.
You can also use the projector to see the word embeddings of these languages:
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/de/projector.json" target="_blank">German</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/el/projector.json" target="_blank">Greek</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/es/projector.json" target="_blank">Spanish</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/fr/projector.json" target="_blank">French</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/fi/projector.json" target="_blank">Finnish</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/hi/projector_v2.json" target="_blank">Hindi</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/id/projector.json" target="_blank">Indonesian</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/it/projector.json" target="_blank">Italian</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/ja/projector.json" target="_blank">Japanese</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/lt/projector.json" target="_blank">Lithuanian</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/pt/projector.json" target="_blank">Portuguese</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/si/projector.json" target="_blank">Sinhalese</a>,
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/sv/projector.json" target="_blank">Swedish</a>, and
<a href="https://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/word-embeddings/zh/projector.json" target="_blank">Chinese</a>.
</p>
<p class="background-information">
Note that it takes several hundred iterations of t-SNE before it settles down on a good mapping from 300 dimensions.
You can also search for words and their neighbors and create bookmarks.
The above links launch the projector with a bookmark showing t-SNE and highlighting the hundred words closes to 'dog'.
</p>

<figure>
<img src="images/projector-bookmarks.png" class="center">
<figcaption>In the lower right corner, you can select projector bookmarks</figcaption></figure>
        
<p class="sample-program">
Here is a program that displays 50 random words at the location generated by t-SNE.

<figure class = "snap-iframe"
        id = "random word locations"
        full_screen = "true"
        container_style = "width: 1000px; height: 800px" 
        caption = "Repeatedly picks a word and displays it at its t-SNE location. TRY IT!">
</figure>
</p>
        
<h4 class="instructions-white" id="generate-projector-files">Generating Projector Files to visualize embeddings</h4>
<p class="background-information">
The blocks <span class="block-name">features of sentence</span> and
<span class="block-name">features of sentences</span> can turn sentences
into points in 512 dimensional space.
<a href="#sentence-embeddings">Learn more here.</a>
While we can't visualize this space there are techniques for mapping these points
into 2 or 3 dimensions that, while approximate, are often insightful.
The <a href="http://projector.tensorflow.org/" target="_blank">TensorFlow Projector</a> implements several such techniques
in a web page.
The projector needs two files: one with the sentence embeddings (i.e. location in the 512D space)
and the other associating the text of the sentence with the embedding.
</p>
<p class="instructions">
The <span class="block-name">&lt;sentences&gt; to TSV</span> block reports the contents of the embeddings file
and the <span class="block-name">&lt;sentences&gt; to TSV metadata</span> block reports the contents of the metadata file.
These blocks and a trivial example of their use can be found in
the  <a href="../snap/snap.html?project=generate projector files&editMode" target="_blank">generate projector files</a> project.
Once these blocks have been run and the results exported as TSV files you can launch the
<a href="http://projector.tensorflow.org/" target="_blank">TensorFlow Projector</a> and
click the <i>Load</i> button to load your files into the projector.
</p>
<p class="instructions">
An example of using these blocks is this
<a href="http://projector.tensorflow.org/?config=https://ecraft2learn.github.io/ai/resources/snap-manual-sentences.json" target="_blank">
application of the TensorFlow Projector to the sentences in the Snap! manual</a>.
After exploring it we suggest you click on the APL bookmark to see one use this projection.
</p>

<h4 class="instructions-white" id="translations">Word embeddings can do translations</h4>
<p class="instructions">
What would happen if you took, for example, the features of the English word 'dog' and 
asked for the closest word in, say, French?
Try this with different source and target languages and different words.
Compare it with <a href="https://translate.google.com/" target="_blank">Google Translate</a>.
Tip: it is easy to copy and paste words that your keyboard can't type from the Google Translate page.
Or you can use the <a href="https://en.wikipedia.org/wiki/Input_method" target="_blank">input method editor</a>
supported by the operating system of your device.
Supported languages are Chinese, English, Finnish, French, German, Greek, Hindi, Indonesian, Italian,
Japanese, Lithuanian, Portuguese, Sinhalese, Spanish, and Swedish.

<figure class = "snap-iframe"
        id = "translate exercise"
        container_style = "width: 800px; height: 460px" 
        caption = "Find the closest word in a different language.">
</figure>
</p>
<p class="instructions">
Note that this version of <span class="block-name">closest word to</span> has
the choice of how one measures the distance between two vectors.
<a href="https://en.wikipedia.org/wiki/Euclidean_distance" target="_blank">Euclidean distance</a>
is the familiar 2D distance measure.
<a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank">Cosine similarity</a>
is similar but preferred by experts.
See if it makes a difference in which words are closest to the untranslated word.
</p>
<p class="advanced-information non-essential">
It is possible to add word embeddings for more languages.
The process is <a href="../word-embeddings/adding-more-languages.html" target="_blank">documented here</a>.
</p>
<h4 class="sample-program-white" id="guess-my-word">A 'Guess My Word' game using word embeddings</h4>
<p class="sample-program">
The following game picks a random word and gives the player warmer or colder feedback as the player makes guesses.
This does so by comparing the distance to the secret word with the previous distance.
It uses the <span class="block-name">location of ...</span> reporter block to display your guesses.
The game is very hard!
There are many ways to make the game better.
See if you can!
</p>

<figure class = "snap-iframe"
        id = "guess my word"
        full_screen = "true"
        container_style = "width: 800px; height: 800px" 
        caption = "Click the green flag to play Guess My Word. TRY IT!">
</figure>
<h4 class="instructions-white" id="question-answering">Question answering</h4>
<p class="background-information">
Natural language processing researchers have been training models to address
sentence-level tasks such as question answering.
They have Sesame Street names such as Elmo, Bert, and Ernie.
<a href="https://blog.tensorflow.org/2020/03/exploring-helpful-uses-for-bert-in-your-browser-tensorflow-js.html"
target="_blank">Bert</a> (really BERT which stands for Bidirectional Encoder Representations from Transformers)
has been made available to browsers.
</p>
<p class="instructions">
BERT is used in the implementation of the <span class="block-name">get up to 5 answers to question</span>
and <span class="block-name">answer question ... using this passage</span> blocks.
Given a passage of text (typically a page or less) they can answer questions about the contents.
The <span class="block-name">get up to 5 answers to question</span> block reports a list whose elements
are lists of answers and a score indicating how confident the model is.
An easy way to import a passage into Snap! is to drag a file that contains the plain text onto the script area.
</p>
<figure class = "snap-iframe"
        id = "question answer"
        full_screen = "true"
        container_style = "width: 800px; height: 800px" 
        caption = "Ask questions of a passage of text describing Snap!. TRY IT!">
</figure>

<h4 class="instructions-white" id="gpt-3 and j1">Getting responses from GPT-3 or Jurassic 1</h4>
<p class="background-information">
<a href="https://en.wikipedia.org/wiki/GPT-3" target="_blank">GPT-3</a> is a neural network model that
is capable of generating text in response to a prompt.
The impressive thing is how often the generated text is very appropriate whether it is
answering questions, fixing the grammar of a sentence, summarizing text, or following instructions.
GPT-3 was trained on hundreds of billions of words from web pages and books.
Surprisingly, the "only" thing it learns is to predict the next word (or token that is sometimes a part of a word).
People call models like this "language models".
</p>

<p class="instructions">
You can access GPT-3 from Snap! using the
<span class="block-name">complete <prompt> using GPT-3 ...</span> block.
You need to provide it with your <a href="https://openai.com" target="_blank">OpenAI GPT-3 API key</a>.
It accepts many of the <a href="https://beta.openai.com/docs/api-reference/completions" target="_blank">
options supported by the OpenAI API</a>.
</p>
<p class="instructions">
AI21 Studios has released a model similar to GPT-3 called Jurassic 1.
The <span class="block-name">complete <prompt> using Jurassic 1 ...</span> block works very much like
the GPT-3 version.
You need to obtain an API key from <a href="https://ai21.com" target="_blank">AI21 Studios</a>.
<a href="https://studio.ai21.com/docs/api/" target="_blank">Jurassic 1 options</a>
are also similar to GPT-3 options but note
that they are implemented by a parallel set of reporters that end with "(J1)".
</p>
<figure class = "snap-iframe"
        id = "GPT-3 exercise"
        container_style = "width: 850px; height: 850px" 
        caption = "See how the different GPT-3 and Jurassic 1 engines complete your prompts">
</figure>

<h4 class="sample-program-white" id="GPT-3 Gopher">Having a conversation with GPT-3 or Jurassic 1</h4>
<p class="sample-program">
<a href="../snap/snap.html?project=GPT-3 Gopher" target="_blank">GPT-3 and Jurassic 1 Gopher</a> is a sample program
that tries to hold a conversation using GPT-3 or Jurassic 1.
It uses <a href="chapter-2.html">speech recognition</a> to listen to what you say.
What you say is added to a prompt that tries to get GPT-3 to be a good conversationalist
by describing the situation and giving it several conversational exchanges.
It avoids the problem that as the conversation goes on the prompt gets too long for GPT-3
by truncating it while keeping the context description.
</p>
<p class="sample-program">
The project supports three "personas". Gopher is an attempt at a friendly general conversationalist.
El is prompted to pretend to be an elephant so one can ask her questions like "What is your favorite food?" or 
"Do you like jogging?".
Eve pretends to be Mount Everest. Ask her questions like "What do you weigh?" or "How can I come visit you?".
Charles is a simple simulation of Charles Darwin.
Ask him about his life, evolution, or his books.
These different personas are running the same script.
The only difference is the introductory text and initial sample conversation.
</p>
<p class="sample-program">
Note that this project also illustrates how one can make a completely speech oriented interface.
Other than the communication of your API key everything uses speech input and output.
This is particularly useful when the project is run on a smartphone.
Look at the script to see how it works.
</p>

<h4 class="instructions-white" id="hugging-face">Using language models on Hugging Face</h4>
<p class="background-information">
<a href="https://huggingface.co" target="_blank">Hugging Face</a> provides access to over 15,000 neural network models.
Free API keys are available that are limited to 30,000 input characters per month
when communicating with text-based models.
Additional input costs $10 per million characters.
See <a href="https://huggingface.co/pricing" target="_blank">Hugging Face pricing</a>.    
</p>
<p class="sample-program">
<a href="../snap/snap.html?project=Hugging Face models&editMode" target="_blank">This Hugging Face project</a>
contains examples of using Hugging Face models to
</p>
<ul>
<li>
Translate between languages. Over 1300 models that translate from language X to language Y are available.
</li>
<li>
Classify text into categories you choose. Optionally text can be given multiple labels.
</li>
<li>
Produce a summary of text. Optionally can control the length, repetition, and more.                
</li>
<li>
Hold a conversation. Note this model is much smaller and less capable than
<a href="#gpt-3 and j1">the GPT-3 and Jurassic 1 models</a>.
</li>
<li>
Query a spreadsheet. Ask questions of a table of data.
</li>
<li>
Answer a question given a context. Note that 
<a href="#question-answering">similar models can be run in the browser</a> without accessing Hugging Face.
</li>
<li>
Detect how positive or negative some text is.
</li>
<li>
Find the named entities in some text.
</li>
<li>
Fill in the blanks. Will predict missing text.
</li>
</ul>
        
<h4 class="sample-program-white" id="codenames">Generating clues for the Codenames board game</h4>
<p class="sample-program">
<a href="https://en.wikipedia.org/wiki/Codenames_(board_game)" target="_blank">Codenames</a>
is an award-winning board game.
Each team has a spymaster who provides a single word clue to his or her teammates.
The teammates see 25 words (initially) and need to guess which ones the spymaster is hinting should be guessed.
By using the <span class="block-name">closest words to</span> reporter one can find a word that is "close"
to the team's words and "far" from the words of the other team or the codeword of the assassin.
</p>
<p class="instructions">
Here is
<a href="../snap/snap.html?project=codenames" target="_blank">a program that partially implements a spymaster</a>.
Given a list of words it finds the best clue for two of those words.
Starting with this program you can make a program for playing <i>Codenames</i>.
Try enhancing it to search for clues for more than two words at a time.
Enhance it to make sure that none of the possible clues suggest the other team's codenames
or the assassin's codename.
</p>
</h4>
<h4 class="societal-impact-white" id="benefits-and-risks">Benefits and risks using word embeddings</h4>
<p class="societal-impact">
Word and sentence embeddings can be used as a component in AI programs that do
sentiment analysis, entity detection, recommendations, summarizing text, translation, and question answering.
This is typically done by replacing the words or phrases in a text with their embeddings
and then training a machine learning system on these 
approximate meaning of the words.
This makes the systems work better with synonyms and paraphrasings.
</p>
<p class="societal-impact">
Word embeddings are learned by examining text with billions of words.
These texts may have captured
<a href="https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html" target="_blank">
societal biases</a>.
For example, the following example seems to have captured the bias that butchers are male and bakers are female.
But the bias is so weak that if <a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank">cosine similarity</a>
is used instead of <a href="https://en.wikipedia.org/wiki/Euclidean_distance" target="_blank">Euclidean distance</a>
the unbiased "chef" is found.
Some word embedding databases have the bias that doctors are male and nurses are female.
They will answer the question "man is to doctor as woman is to X" with "nurse".
Is this a bias? Or might it be due to the fact that only women can <i>nurse</i> babies?
Run some experiments below to explore these kinds of questions.
</p>
<figure class = "snap-iframe"
        id = "word analogy bias"
        container_style = "width: 700px; height: 450px" 
        caption = "Bias in word analogy solutions. Run it and then swap man and woman. See if you can find more biases.">
</figure>
<p class="societal-impact">
A paper called <a href="https://science.sciencemag.org/content/356/6334/183" target="_blank">
Semantics derived automatically from language corpora contain human-like biases</a>
proposed a way to measure word biases.
The idea is to use the average distance two words have to two sets of "attribute" words.
To explore gender bias, for example, the attribute word lists can be "male, man, boy" and "female, woman, girl".
The difference of the average distances provides a score that can be used to compare words.
In <a href="../snap/snap.html?project=word%20bias&editMode&noRun" target="_blank">
this implementation of the scoring reporter</a> one can see that "mathematics" has a higher "maleness" score than "art".
And "art" has a higher "pleasantness" score than "mathematics".
</p>
<h4 class="how-it-works-white" id="how-do-word-embeddings-work">How do word embeddings work?</h4>
<p class="how-it-works">
While we don't really know what the numbers mean, they must be encoding lots of things about words such as
gender, grammatical category, family relationships, and hundreds more things.
But the numbers aren't perfect.
See if you can create some examples where the results are not good.
One known problem with how the numbers are generated is that it combines features of different senses of the same word.
There is only one entry, for example, for 'bank' which combines the ways that word is
used in sentences about financial institutions and those about the sides of rivers.
This can cause words to be closer than they should be.
For example, "rat" and "screen" end up being closer together than otherwise due to "rat" being close to "mouse"
and "mouse" (the computer input device) being close to (computer) "screen".
This is a problem researchers are working on.
Another problem is that sometimes short phrases act like words.
"Ice cream", for example, has no word embedding while "sherbet" and "sorbet" do.
</p>
<h4 class="sample-program-white" id="translation-sample-project">A sample project using word embeddings for translation</h4>
<p class="sample-program">
Here is
<a href="../snap/snap.html?project=translate&noRun" target="_blank">a program that asks the user for two languages</a>,
obtains the feature vector of a random word from the first language,
and then displays several of the words closest to that feature vector.
It places the words in the <a href="https://distill.pub/2016/misread-tsne/" target="_blank">t-SNE</a>
two-dimensional approximation of where the 300-dimensional words really are.
</p>
<h4 class="how-it-works-white" id="how-does-translation-work">How does translation using word embeddings work?</h4>
<p class="how-it-works">
The word embeddings for each language were generated independently based upon text from Wikipedia and the web.
The location in 300-dimensional space of the features for a word like 'dog' have no relationship 
to the location of features of translations of the word 'dog'.
<a href="https://arxiv.org/abs/1309.4168" target="_blank">Researchers</a>
noticed that in most (all?) languages some words are close together.
For example, 'dog', 'dogs', puppy', and 'canine' are close.
Words like 'wolf', 'cow', and 'mouse' are close but not as close as those words.
And all of these words are far from abstract words like 'truth' and 'logic'.
So they discovered that it is possible to find a rotation that will cause many word embeddings
in one language to be close to corresponding words in another language.
The way it was done at first and in these Snap! blocks is by giving a program a word list between
English and each other language.
500 words is enough to find a good rotation that brings most of the other 19500 words
close to where their translations are.
While it is impressive that translation works at all given a word list that covers only 2.5% of the vocabulary,
<a href="https://engineering.fb.com/ai-research/unsupervised-machine-translation-a-novel-approach-to-provide-fast-accurate-translations-for-more-languages/" target="_blank">
Researchers at Facebook describe a technique</a> that uses no word lists or translated texts.
A rotation is all that is needed because all the word embeddings are centered around zero
so they don't need to be translated (in the mathematical sense, i.e. moved) as well.
But note that the translation happens in 299 dimensions!
</p>
<figure>
<img src="images/aligning-word-embeddings.png" class="center">
<figcaption>Two ways of aligning word embeddings in different languages</figcaption></figure>
<p class="how-it-works">
In the figure (A) and (B) show X being rotated to match Y to make a small number of words in X align with their translation in Y.
Many other words become roughly aligned as a result.
<a href="https://arxiv.org/pdf/1710.04087.pdf" target="_blank">Other techniques can be applied</a> to improve the alignment.
</p>
<h4 class="instructions-white" id="image_embeddings">Image embeddings are possible as well</h4>
<p class="instructions">
Using a similar technique to how vectors are generated for words we can also generate vectors for images.
The <span class="block-name">get costume features of ...</span> block will pass
a vector of 1280 numbers to the blocks provided.
This uses MobileNet to compute the numbers from the "top" of the neural net.
</p>
<figure class = "snap-iframe"
        id = "image embedding exercise"
        container_style = "width: 675px; height: 525px" 
        caption = "A block for converting images into lists of 1280 numbers. TRY IT!">
</figure>
<p class="instructions">
One can use image embeddings to determine which images are close to other images.
Closeness takes into account many factors including texture, color, parts, and semantics.
Image embeddings can be used to work out image analogy problems similar to how word analogy problems are solved.
</p>
<p class="how-it-works">
In the machine learning chapter there is a
<a href="chapter-4.html#image-recognition-without-cloud-services" target="_blank">description</a>
of the <span class="block-name">train with image buckets ...</span> block which is used for training.
It works by collecting the feature vectors of all the training images and then finds the nearest neighbors to 
a test image to determine what label to give the image.
</p>
<h4 class="project-ideas-white" id="project-ideas">Possible project ideas for Natural Language Processing</h4>
<p class="project-ideas">
Here are some project ideas:
</p>
<ol class="project-ideas">
<li>
Find a chain of similar words by finding the nearest word to the starting word.
Then repeatedly find the nearest word to that while making sure to never repeat the same word.
Use this to repeatedly change random words one at a time in a famous poem or text
(e.g. "roses are red and violets are blue").
</li>
<li>
Make word games using word embeddings.
For example, something like <a href="https://experiments.withgoogle.com/semantris" target="_blank">Semantris</a>,
a semantic version of Tetris.
A bilingual version Semantris might be a good idea.
</li>
<li>
Create a program that searches for new word analogies.
Hint: If A is to B as C is to D, then A-B "is close to" C-D.
</li>
<li>
Explore why sometimes word analogies are right and sometimes wrong.
Does the second, third, or tenth closest answer make more sense?
Hint: use the <span class="block-name">closest words to</span> reporter to explore this.
Is it better at word analogies when A is close to B in "A is to B as C is to D"?
</li>
<li>
<a href="https://www.sciencedaily.com/releases/2017/04/170413141055.htm" target="_blank">Researchers have found</a>
that if you look at the average distance between pleasant words and the word 'flowers', the distance is much smaller than
the distance of 'flowers' to unpleasant words.
The opposite holds if you explore the distance of pleasant and unpleasant words to the word 'insects'.
Based upon this observation people have made other comparisons to see how, for example,
words about males are closer to words about science while words about females are closer to art words.
See if you can find other biases that arise from the way people write about things.
If you know another language (and it is one of the 15 supported languages) see if it applies across languages.
</li>
<li>
Explore the relationships between words in groups such a color words or emotion words. 
Are 'red', 'green', and 'blue' close to each other or are color words close when the colors appear similar.
If you know another language, see if different languages organize color words differently.
Try this with other categories such emotions, directions, numbers, etc.
</li>
<li>
Explore the relationships between different grammatical versions of the same word stem.
E.g. the relationships of 'tall', 'taller', 'tallest', 'short', 'shorter', 'shortest'.
Or 'run', 'ran', 'running' (and 'runny' and 'runnier').
Can you find some general patterns?
Do they generalize to other languages?
</li>
<li>
Explore word embeddings for number words.
For example, "two" is to "four" as "ten" is to "twenty". Can you find other examples?
</li>
<li>
The <a href="#guess-my-word">Guess My Word</a> is very difficult because the answer can be one of 20,000 words.
Make an easier version of the game.
</li>
<li>
Implement a spymaster for the <a href="#codenames">Codenames</a> game.
</li>
<li>
Try using embeddings to explore the similarity of sentences.
Use the <span class="block-name">features of sentences ...</span> block to get the features of sentences.
Create a project that decides if two sentences are paraphrases.
</li>
<li>
Create a question answering project that checks if the question being asked if similar enough to
a known question so it can reply with a "canned" answer.
You might want to start by improving <a href="chapter-6.html#ai-questions">
the sample project that answers questions about Snap! AI blocks</a>
</li>
<li>
Using the <span class="block-name">answer this question ... using this passage ...</span> block
together with the <span class="block-name">Ask Wikipedia</span> block to make a general question answering program.
Consider adding speech input and output.
</li>
<li>
There are many ways you can add GPT-3 to your projects by using the 
<span class="block-name">complete <prompt> using GPT-3 ...</span> block.
OpenAI lists <a href="https://beta.openai.com/examples" target="_blank">dozens of tasks</a> you can give it.
</li>
<li>
The <a href="../snap/snap.html?project=GPT-3 Gopher" target="_blank">GPT-3 and Jurassic 1 sample project</a>
supports three personas. See if you can invent more.
For example, consider
<a href="https://www.grandcentralpublishing.com/titles/moiya-mctier/the-milky-way/9781538754153/" target="_blank">the Milky Way</a>.

</li>
<li>
Be creative! Word embeddings and language models are new and there is much that remains to discover.
</li>
</ol>

<h4 class="background-information-white" id="future-directions">Future directions for this chapter</h4>
<p class="background-information">
New word embeddings blocks could be added.
Currently the word embeddings blocks excludes all proper nouns.
If we added them one could solve analogies such as Paris is to France as Berlin is to X.
Exploring how words change over time can lead to great projects.
<a href="https://nlp.stanford.edu/projects/histwords/" target="_blank">
Word embeddings generated from publications in different time periods</a>
could be used to see how words like "awful" and "broadcast" have changed over the last two centuries.
New blocks could be added based upon research on generating
<a href="https://scholar.google.co.uk/scholar?hl=en&as_sdt=0%2C5&q=%22word+sense%22+embeddings&btnG=" target="_blank">
"word sense" embeddings</a>
instead of word embeddings.
E.g. one sense of "duck" is close to "chicken" while another sense is close to "jump".
</p>
<p class="background-information">
There is plenty more that AI programs can do with language including
determining the grammatical structure of sentences (this is called "parsing"),
figuring out the sentiment in some text,
and summarizing text.
We plan to add more.
</p>

<h4 class="resources-white" id="additional-resources">Additional resources</h3>
<p class="resources">
Wikipedia's <a href="https://en.wikipedia.org/wiki/Word_embedding" target="_blank">word embedding</a> article is short and
written for an advanced audience.
The Facebook team wrote a paper detailing how they generated the word embeddings we used here:
E. Grave, P. Bojanowski, P. Gupta, A. Joulin, T. Mikolov,
<i><a href="https://arxiv.org/abs/1802.06893" target="_blank">Learning Word Vectors for 157 Languages</a></i>.
The <a href="https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469" target="_blank">Understanding word vectors</a> web page
has a very good introduction to the subject and the contains examples that are helpful but require familiarity with Python.
This <a href="https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html" target="_blank">
Google blog about biases in word embeddings</a> is very good and clear.
Google's <a href="https://cloud.google.com/blog/products/ai-machine-learning/steering-the-right-course-for-ai" target="_blank">
Steering the right course for AI</a> discusses bias along with other societal issues including interpretivity, jobs, and doing good.
<a href="https://arxiv.org/abs/1905.09866" target="_blank">Fair is Better than Sensational:
Man is to Doctor as Woman is to Doctor</a> discusses biases in word embeddings in depth.
<a href="https://distill.pub/2016/misread-tsne/" target="_blank">How to Use t-SNE Effectively</a>
is a clear interactive description of how t-SNE works.
<a href="https://arxiv.org/abs/1309.4168" target="_blank">Exploiting Similarities among Languages for Machine Translation</a>
pioneered the idea adjusting the word embeddings to support translation.
<a href="https://arxiv.org/abs/1710.04087" target="_blank">Word Translation Without Parallel Data</a>
explores how word embeddings can be used for translation without using word lists or translated texts.
<a href="https://projector.tensorflow.org/" target="_blank">projector.tensorflow.org</a>
is a great website for interactively exploring different ways of visualizing high-dimensional spaces.
Here is a video of <a href="https://www.youtube.com/watch?v=RJVL80Gg3lA" target="_blank">
a nice talk by Laurens van der Maaten</a> who invented the idea of t-SNE.
</p>
<h4 class="guide-to-guide-white">Where to get these blocks to use in your projects</h4>
<p class="resources">
You can import the blocks presented here as a
<a href="/ai/snap/snap.html?project=words&editMode" target="_blank">project</a> or 
download them as a <a href="words blocks.xml" download target="_blank">library</a> to import into your projects.
</p>
<h4 class="guide-to-guide-white" id="making-neural-nets">Learn about making and training neural nets</h4>
<p class="guide-to-guide">
Go to
<a class="guide-link" href="chapter-6.html">the next chapter on neural nets</a>
</p>
<p class="guide-to-guide">
Return to 
<a class="guide-link" href="chapter-4.html">the previous chapter on machine learning</a>.
</p>
<script src="../js/bottom-of-page.js"></script>
</body>
</html>