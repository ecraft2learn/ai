<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>Camera Input - AI Guide</title>
<link href="/ai/css/ai-teacher-guide.css" rel="stylesheet">
<link href="/ai/css/oer-style.css" rel="stylesheet">
<link rel="icon" type="image/png" href="/ai/images/eCraft2Learn-Favicon.png" />
<script src="/ai/js/ai-guide.js"></script>
</head>
<body>
<script src="/ai/js/translate.js"></script>
<script src="/ai/js/top-of-page.js"></script>
<h3>Chapter 3 - Camera Input</h3>
<h4>Ken Kahn, University of Oxford</h4>
<h3>Browser compatibility</h3>
<p>This chapter of the guide includes many interactive elements that currently only run well in the Chrome browser.
This chapter relies upon there being a camera that the browser can access.
There is a <a href="troubleshooting.html" target="_blank">troubleshooting guide</a>
that should be consulted if problems are encountered.</p>

<h3>Introduction</h3>
<p>A camera connected to a computer can report the colour of each pixel in an image and not much else.
A description of what is in front of the camera is returned when those pixels
are sent to an image recognition service.
There are many kinds of things an image description may contain.
With speech recognition the description is what was spoken, how confident the system is, and possible alternatives.
With image recognition there are many possible descriptions: descriptive tags, possible captions, dominant colours,
location of faces and parts of the face if there are any,
and the presence of landmarks, celebrities, well-known entities, and logos.
Hand-written and scanned text can also be recognised.</p>

<p>A challenge in providing student-friendly programming blocks for image recognition is that
different AI cloud services report different descriptions in different ways.
We currently provide interfaces to image recognition services provided by Google, Microsoft, and IBM.
A further challenge is how to provide simple interfaces for simple tasks
while still supporting more sophisticated uses and projects.</p>

<p>In the last few years there has been tremendous progress in computer vision.
There are high performance systems for identifying objects, recognising faces, interpreting sketches,
and using medical images to aid diagnosis.
Driverless cars rely heavily upon computer vision.</p>

<h4>API keys are needed</h4>
<p>Chrome has builtin support for speech input and output.
No browser, however, currently supports image recognition.
To access vision recognition services from companies such as Google, Microsoft, or IBM
one needs to <a href="https://github.com/ToonTalk/ai-cloud/wiki" target="_blank">open an account</a>.
Accounts are free and provide some degree of free usage.
IBM permits 250 free vision queries per day, Microsoft 5000 per month, and Google 1000 per month.
To try the vision blocks described here you need at least one account.
Comparing and contrasting the results from different services is an interesting ways
to gain some insight into how these services work.</p>

<p>In projects there is a different way of providing keys to Snap!
(as described <a href="#api-keys">here</a>) but in this chapter you can paste your key or keys below
and they'll be passed to the services as you use example blocks.
<br><label>Copy and paste your Google key here:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<input id="Google image key" type="text"></label>
<br><label>Copy and paste your Microsoft key here:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<input id="Microsoft image key" type="text"></label>
<br><label>Copy and paste your IBM Watson key here: <input id="IBM Watson image key" type="text"></label></p>

<h4>A simple image recognition block</h4>
<p>This block takes a picture, sends it to the AI vision cloud server provider, waits for a response, and then
reports a list of labels of the photo.
The list is ordered by how confident the vision provider is that the label matches the image.</p>

<figure>
<div class="iframe-container"
     style="width: 800px; height: 350px;">
<iframe class="iframe-clipped"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/image labels reporter.xml"> 
</iframe></div>
<figcaption>A block for reporting the labels describing what the camera is showing. TRY IT</figcaption>
</figure>

<h4>Displaying the image that was sent for recognition</h4>
<p>The <span class="block-name">show current photo</span> block will display
the most recent image sent to the specified AI vision services
as the background of the Snap! stage.
To try it first obtain some labels from a vision service.
You can also use the <span class="block-name">use camera to create costume</span> block
to take a new picture and add it as a costume to the current sprite.</p>

<figure>
<div class="iframe-container" style="width: 1000px; height: 350px;">
<iframe class="iframe-clipped"
        style="margin-left: -800px; margin-top: 150px;"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/image labels reporter and show photo.xml"> 
</iframe></div>
<figcaption>After sending a photo for analysis you can use a block to display the image. TRY IT</figcaption>
</figure>

<h4>A sample program using image recognition</h4>
<p>Here is a program that can contact any of Google, Microsoft, or IBM and displays the tags returned.</p>

<figure>
<iframe style="width: 800px; height: 600px"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/camera and say.xml"
        run_full_screen="true"> 
</iframe></div>
<figcaption>Click on the AI cloud provider logos and see what happens</figcaption>
</figure>

<h4>A sample program combining image and speech recognition and speech output</h4>
<p>This program is similar to the previous one except it is listening for the words "Google", "Microsoft", or "Watson".
When it hears one of these words it contacts that service.
When the response is received it turns it into speech.
As a demo it is impressive if you say things like "Tell me Google what do you see?" and "What do you see Microsoft?".
This will work in every language.
It is a nice illustration of how software can appear more intelligent than it is.
<span class="teacher-guide">This could be a good topic for a student discussion on how and
where this program is intelligent and where it isn't.</span>
<span class="student-guide">Do you think this program <i>seems</i> intelligent? If so, why? Is it intelligent?</span>
</p>

<figure>
<iframe style="width: 800px; height: 600px"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/camera, listen, and speak.xml"
        full_screen="true"> 
</iframe></div>
<figcaption>Click the green flag and then say a sentence containing "Google", "Microsoft", or "Watson"</figcaption>
</figure>

<h4 id="advanced-image-recognition">Advanced image recognition blocks</h4>
<p>Image recognisers can do more than label images.
Some can detect and locate faces.
Some of those will estimate the age of the person and their gender.
Some recognise landmarks and logos.
Many can recognise characters in hand-written or scanned text.
A problem creating Snap! blocks that provide this functionality is that
different services have different capabilities and different structures to capture their responses.
</p>

<b class="student-guide advanced-topic">Click to read an advanced topic</b>
<span>
<h4 id="current-image-properies">Getting properties of an image</h4>
<p>
The <span class="block-name">Recognize new photo</span> block addresses this problem
by reporting a data structure capturing the entire response from
the vision service.
It is a Snap! list that contains lists that contains more data that might be text for labels,
numbers for confidence scores, and even more lists for complex data.
You can double click on the <img src="images/sub-list-icon.png" alt="icon for opening lists in lists">
icons to drill down the structure.
</p>
<p>
The <span class="block-name">Current image property</span> block
takes an argument that describes what piece of the response structure
should be reported.
For example, Microsoft offers possible captions that can be found by following the path
<span class="block-name">"description captions text"</span>.
Common useful blocks are defined that use
the <span class="block-name">Current image property</span> block internally
to get the labels and the confidence scores of the labels for each of the supported vision service providers.
Note that after a response is received from any of the AI services it is stored so that calls to
<span class="block-name">Current image property</span> use the most recent response rather than ask for a new one.
This is because a project may need to access multiple pieces of a response.
</p>
<p>
After having run <span class="block-name">Recognize new photo</span> block
you should select the same AI cloud provider
(currently Google, Microsoft, or IBM Watson)
when you run the <span class="block-name">Current image property</span> block.
The second argument can be a string or a list of strings specifying what information
is desired from the image recognition.
Each AI cloud provider supports different image properties:
<ul>
<li>
<b>IBM Watson.</b> Currently the only properties supported are
<span class="block-name">class</span> and <span class="block-name">score</span>.
More details
<a href="https://www.ibm.com/watson/developercloud/visual-recognition/api/v3/#classify_an_image" target="_blank">here</a>.
</li>
<li>
<b>Google.</b> It supports properties that are lists of the form <span class="block-name">[labelAnnotations description]</span>,
<span class="block-name">[labelAnnotations score]</span> and <span class="block-name">[imagePropertiesAnnotation dominantColors colors]</span>.
More details
<a href="https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate#AnnotateImageResponse" target="_blank">here</a>.
</li>
<li>
<b>Microsoft.</b> It supports properties that are lists of the form
<span class="block-name">[Categories name]</span>,
<span class="block-name">[Categories score]</span>,
<span class="block-name">[Description captions text]</span>,
<span class="block-name">[Description tags]</span>,
<span class="block-name">Faces</span>,
<span class="block-name">[Tags name]</span>, and
<span class="block-name">[Tags confidence]</span>.
More details
<a href="https://westcentralus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa" target="_blank">here</a>.
</li>
</ul>
</p>
<p>
Common calls to the get property reporter are pre-defined.
They are <span class="block-name">IBM Watson classes</span>, <span class="block-name">IBM Watson scores</span>,
<span class="block-name">Microsoft labels</span>, <span class="block-name">Microsoft first caption</span>,
<span class="block-name">Google labels</span>, and <span class="block-name">Google label scores</span>.
</p>
</span>

<p class="student-guide">
<b>Exercise.</b>
Recognise a picture and then use
the <span class="block-name">Current image property</span> block to see
what kinds of descriptions the service provides.
How might each one be useful?
</p>

<p><b class="student-guide">Advanced Exercise.</b>
Compare the documentation for the vision services from
<a href="https://cloud.google.com/vision/" target="_blank">Google</a>,
<a href="https://azure.microsoft.com/en-gb/services/cognitive-services/computer-vision/" target="_blank">Microsoft</a>,
and
<a href="https://www.ibm.com/watson/services/visual-recognition/" target="_blank">IBM Watson</a>.</p>

<figure>
<div class="iframe-container" style="width: 800px; height: 350px;">
<iframe class="iframe-clipped"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/recognize new photo.xml"> 
</iframe></div>
<figcaption>A block for reporting the entire analysis response. TRY IT</figcaption>
</figure>

<b class="student-guide advanced-topic">Click to read an advanced topic</b>
<span>
<p>
The <span class="block-name">Recognize new photo</span> reporter block is implemented
using the <span class="block-name">Ask &lt;provider&gt; to say what it sees</span> block.
This block does not wait for a response, instead it runs the user's blocks when a response is received.
It isn't as convenient as 'Recognize new photo' but it can support more complex usage.
</p>

<figure>
<div class="iframe-container" style="width: 800px; height: 350px;">
<iframe class="iframe-clipped"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/recognize asynchronous.xml"> 
</iframe></div>
<figcaption>A block for obtaining the entire analysis response as a callback. TRY IT</figcaption>
</figure>
</span>

<h4 id="api-keys">How to provide API keys</h4>
<p>This chapter uses the API keys provided in the text areas above.
When constructing a project there are two ways to provide keys:
<ol>
<li>Add extra information to the page's URL.
Appending any (or all) of
<span class="block-name">&IBM Watson image key=..</span> or
<span class="block-name">&Google image key=...</span> or
<span class="block-name">&Microsoft image key=...</span>
to a shared project URL will
provide all three keys (after "..." is replaced by the real keys).
You can provide only one key if you aren't interested in comparing the responses from different AI service providers.
Refresh the page after adding the keys.
</li>
<li>Edit any of the following special reporters (found in the <a href="#library">eCraft2Learn block library</a>):</li>
</ol>

<figure>
<div class="iframe-container" style="width: 800px; height: 170px;">
<iframe class="iframe-clipped"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/blank keys.xml"> 
</iframe></div>
<figcaption>Reporters for API keys that users edit to provide service keys.</figcaption>
</figure>

<p class="teacher-guide">Because all the AI cloud services are commercial services heavy use can be costly.
Consequently it is best if each student has their own account and minimises the sharing of their keys.
This conflicts with one of the great things about tools like Scratch and Snap! --
that it is so easy to share one's projects with a wider community.
Adding the keys to the URL solves this problem if one is careful to keep the URL with keys private
and share the version without the keys.
It is possible but more awkward to maintain private and public versions when using reporters for keys.</p>

<h4 id="image-recognition-good-for">What is image recognition good for?</h4>
<p>This is like asking what is vision good for.
According to <a href="https://en.wikipedia.org/wiki/Evolution_of_the_eye" target="_blank">Wikipedia</a>
eyes have independently evolved between 50 and 100 times since animals first appeared.
Computer vision enables robots to see, self-driving cars (which really are just a kind of robot),
new kinds of user interfaces, support for doctors, police, sports coaches, farmers, the military, and more.
It can help blind people to navigate their surroundings and to use devices.
</p>

<h4 id="image-recognition-dangers">What are the dangers of image recognition?</h4>
<p>Like much of AI technology it may take away many jobs.
It may reduce our privacy since it makes it much easier to track people's movements and activities.
It can enable autonomous weapons that may kill many.</p>

<p>Another danger is that either intentionally or unintentionally the data
the machine learning system is trained on may contain biases.
Here is a nice short video on bias and machine learning published by Google.</p>
<iframe width="560" height="315"
        src="https://www.youtube.com/embed/59bMh59JQDo"
        frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

<h4 id="how-does-computer-vision-work">How does computer vision work?</h4>
<p>Image recognition begins with pixels.
Each pixel is a number or three numbers (for red, green, and blue components)
that corresponds to a tiny piece of an image.
There are two main approaches to processing images:
<ol>
<li>A sequence of programmed processing steps to find edges, determine textures, identify objects, etc.</li>
<li>A machine learning system</li>
</ol>

Most of the recent progress has been with machine learning systems that for many tasks are near human level.
For some tasks such as
<a href="https://www.extremetech.com/extreme/233746-ai-beats-doctors-at-visual-diagnosis-observes-many-times-more-lung-cancer-signals" target="_blank">
detecting lung cancer from x-rays</a> machine learning has exceeded the ability of experts.</p>

<p>A neural net is a program that is inspired by what little is known
about how neurons in animal and human brains work.
It can be trained to identify images.
In the most common case it is trained on thousands or millions of images that have already been labelled.
When given a new image and it computes the most likely labels.
Some easy visual discrimination tasks can be handled by training only a hundred or less images.
You can train a neural net to do this kind of vision recognition in the
<a class="guide-link" href="chapter-4.html">next chapter</a>.
</p>

<p>Different levels or stages of the processing by the neural net recognise different elements of an image.
Researchers have built
<a href="https://distill.pub/2017/feature-visualization/" target="_blank">
software that figures out what images produce the greatest response from the different layers</a>.
</p>

<figure>
<img src="images/edges layer.png" class="center">
<figcaption>Ideal images for a neural net layer that looks for edges</figcaption></figure>
<figure>
<img src="images/textures layer.png" class="center">
<figcaption>Ideal images for a neural net layer that looks for textures</figcaption></figure>
<figure>
<img src="images/patterns layer.png" class="center">
<figcaption>Ideal images for a neural net layer that looks for patterns</figcaption></figure>
<figure>
<img src="images/parts layer.png" class="center">
<figcaption>Ideal images for a neural net layer that looks for parts</figcaption></figure>
<figure>
<img src="images/objects layer.png" class="center">
<figcaption>Ideal images for a neural net layer that looks for objects</figcaption></figure>

<h4>What about <i>video</i> recognition services?</h4>
<p>Some AI cloud providers such as
<a href="https://cloud.google.com/video-intelligence/" target="_blank">Google</a>
can accept a video stream and produce labels.
It can also detect scene changes.
This service can be expensive, though Google will analyse 1000 minutes of video per month at no cost.
There currently are no Snap! blocks for supporting video input.</p>

<h4 id="project-ideas">Possible project ideas using image recognition</h4>
<p>Adding image recognition to a robotics project can make the robot behave much more intelligently.
It can head towards goals and avoid specified objects.
A simple example would be a robot told to move to X
(where X can be "the red ball", "the toy truck", "a person", or whatever).
It begins by sending the current camera image for recognition.
If the description returned matches X then go forward a few steps
(assuming the camera is mounted pointing forward).
If not then turn a little and try again.
Repeat until X is reached.
</p>
<p>For projects that use a camera that can't move there are many possibilities:
<ol>
<li>Engages in a simple conversation about what it thinks it is seeing.</li>
<li>Responds to what it sees.
E.g. says "How cute" when the description includes words such as "kitten" or "puppy"
but says "How scary!" when the description is a toy lion or wolf.</li>
<li>Enhance the <a class="guide-link" href="chapter-2.html#story-generator" target="_blank">story generator example</a>
so that in addition to tokens that ask for phrases or names, a new kind of token
is introduced that causes the current image to be recognised and
then substitutes the resulting description into that location of the story.</li>
<li>When a face is recognised its location and the location of parts of the face are included in the response. 
The app can then add know where to place glasses, mustaches, etc. to the image.</li>
<li>And thousands more possibilities.</li>
</ol>

<b class="student-guide advanced-topic">Click to read an advanced topic</b>
<span>
<h3 id="additional-resources">Additional resources</h3>
<a href="https://cloud.google.com/vision/" target="_blank">Google</a>,
<a href="https://azure.microsoft.com/en-gb/services/cognitive-services/computer-vision/" target="_blank">Microsoft</a>,
and
<a href="https://www.ibm.com/watson/services/visual-recognition/" target="_blank">IBM Watson</a>
document their AI vision services.
There are many more vision services, see for example this
<a href="https://www.upwork.com/hiring/data/comparing-image-recognition-apis/" target="_blank">image recognition services comparison page</a>.
The <a href="https://en.wikipedia.org/wiki/Computer_vision" target="_blank">Wikipedia page</a> on computer vision is very thorough.
There are many MOOCS on computer vision and machine learning for image recognition.
<a class="notranslate" translate=no href="https://distill.pub/" target="_blank">Distill</a> is a scientific journal that strives to explain clearly complex machine learning topics.
Codecademy has a <a href="http://news.codecademy.com/what-is-machine-learning/" target="_blank">
good interview with a data scientist about machine learning</a>.
</p>
</span>

<script src="/ai/js/where-to-get-library.js"></script>

<h3>Learn about machine learning</h3>
<p>The <a class="guide-link" href="chapter-4.html">next chapter</a>
is about machine learning.</p>
<p>
Return to
<a class="guide-link" href="chapter-2.html">the previous chapter on speech input</a>.
</p>

<script src="/ai/js/bottom-of-page.js"></script>

</body></html>