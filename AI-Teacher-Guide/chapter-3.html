<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>Camera Input - AI Teacher's Guide</title>
<link href="ai-teacher-guide.css" rel="stylesheet">
<link rel="icon" type="image/png" href="images/eCraft2Learn-Favicon.png" />
</head>
<body>
<h2>A teacher's guide to helping students build AI apps and artefacts</h2>
<h3>Chapter 3 - Camera Input</h3>
<h4>Ken Kahn, University of Oxford</h4>
<h3>Browser compatibility</h3>
<p>This chapter of the guide includes many interactive elements that currently only run well in the Chrome browser.
This chapter relies upon there being a camera that the browser can access.
There is a <a href="troubleshooting.html" target="_blank">troubleshooting guide</a>
that should be consulted if problems are encountered.</p>
<h3>Introduction</h3>
<p>A camera connected to a computer can report the colour of each pixel in an image and not much else.
A description of what is in front of the camera is returned when those pixels are sent to an image recognition service.
There are many kinds of things an image description may contain.
With speech recognition the description is what was spoken, how confident the system is, and possible alternatives.
With image recognition there are many possible descriptions: descriptive tags, possible captions, dominant colours,
location of faces and parts of the face if there are any, and the presence of landmarks, celebrities, well-known entities, and logos.
Text can also be recognised.</p>

<p>A challenge in providing student-friendly programming blocks for image recognition is that
different AI cloud services report different descriptions in different ways.
We currently provide interfaces to image recognition services provided by Google, Microsoft, and IBM.
A further challenge is how to provide simple interfaces for simple tasks while still supporting more sophisticated uses and projects.</p>

<p>In the last few years there has been tremendous progress in computer vision.
High performance systems for identifying objects, recognising faces, interpreting sketches, and using medical images to aid diagnosis.
Driverless cars rely heavily upon computer vision.</p>

<h4>API keys are needed</h4>
<p>Chrome has builtin support for speech input and output.
No browser, however, currently supports image recognition.
To access vision recognition services from companies such as Google, Microsoft, or IBM
one needs to <a href="https://github.com/ToonTalk/ai-cloud/wiki" target="_blank">open an account</a>.
Accounts are free and provide some degree of free usage.
IBM permits 250 free vision queries per day, Microsoft 5000 per month, and Google 1000 per month.
To try the vision blocks described here you need at least one account.
Comparing and contrasting the results from different services is an interesting way to gain some insight into how these services work.</p>

<p>In projects there is a different way of providing keys to Snap! (as described <a href="#api-keys">here</a>) but in this chapter you can paste your keys below
and they'll be passed to the services as you use example blocks.
<br><label>Copy and paste your Google key here:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<input id="Google image key" type="text"></label>
<br><label>Copy and paste your Microsoft key here:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<input id="Microsoft image key" type="text"></label>
<br><label>Copy and paste your IBM Watson key here: <input id="IBM Watson image key" type="text"></label></p>

<h4>A simple image recognition block</h4>
<p>This block takes a picture, sends it to the AI vision cloud server provider, waits for a response, and then
reports a list of labels of the photo.
The list is ordered by how confident the vision provider is that the label matches the image.</p>

<figure>
<div class="iframe-container"
     style="width: 800px; height: 350px;">
<iframe class="iframe-clipped"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/image labels reporter.xml"> 
</iframe></div>
<figcaption>A block for reporting the labels describing what the camera is showing. TRY IT</figcaption>
</figure>

<h4>Displaying the image that was sent for recognition</h4>
<p>The 'show current photo' block will display the most recent image sent to the specified AI vision services
as the background of the Snap! stage.
To try it first obtain some labels from a vision service.
The 'use camera to create costume' block takes a new picture and adds it as a costume to the current sprite.</p>

<figure>
<div class="iframe-container" style="width: 1000px; height: 400px;">
<iframe class="iframe-clipped"
        style="margin-left: -500px; margin-top: 200px;"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/image labels reporter and show photo.xml"> 
</iframe></div>
<figcaption>After sending a photo for analysis you can use a block to display the image. TRY IT</figcaption>
</figure>

<h4>A sample program using image recognition</h4>
<p>Here is a program that can contact any of Google, Microsoft, or IBM and displays the tags returned.</p>

<figure>
<iframe style="width: 800px; height: 600px"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/camera and say.xml"
        run_full_screen="true"> 
</iframe></div>
<figcaption>Click on the AI cloud provider logos and see what happens</figcaption>
</figure>

<h4>A sample program combining image and speech recognition and speech output</h4>
<p>This program is similar to the previous one except it is listening for the words "Google", "Microsoft", or "Watson".
When it hears one of these words it contacts the service.
When the response is received it turns it into speech.
As a demo it is impressive if you say things like "Tell me Google what do you see?" and "What do you see Microsoft?".
It is a nice illustration of how software can appear more intelligent than it is.
Though this could be a good topic for a student discussion on how and where this program is intelligent and where it isn't.
</p>

<figure>
<iframe style="width: 800px; height: 600px"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/camera, listen, and speak.xml"
        full_screen="true"> 
</iframe></div>
<figcaption>Click the green flag and then say a sentence containing "Google", "Microsoft", or "Watson"</figcaption>
</figure>

<h4>Advanced image recognition blocks</h4>
<p>Image recognisers can do more than label images.
Some can detect and locate faces.
Some of those will estimate the age of the person and their gender.
Some recognise landmarks and logos.
Many can recognise characters in text.
A problem creating Snap! blocks that provide this functionality is that
different services have different capabilities and different structures to capture their responses.
Compare the documentation for the vision services from
<a href="https://cloud.google.com/vision/" target="_blank">Google</a>,
<a href="https://azure.microsoft.com/en-gb/services/cognitive-services/computer-vision/" target="_blank">Microsoft</a>,
and
<a href="https://www.ibm.com/watson/services/visual-recognition/" target="_blank">IBM Watson</a>.</p>

<p>The 'Recognize new photo' block addresses this problem by reporting a data structure capturing the entire response from
the vision service.
It is a Snap! list that contains lists that contains more data that might be text for labels,
numbers for confidence scores, and even more lists for complex data.
You can double click on the <img src="images/sub-list-icon.png" alt="icon for opening lists in lists"> icons to drill down the structure.
The 'Current image property' block takes an argument that describes what piece of the response structure should be reported.
For example, Microsoft offers possible captions that can be found by following the path "description captions text".
Common useful blocks are defined that use the 'Current image property' block internally
to get the labels and the confidence scores of the labels for each of the supported vision service providers.
Note that after a response is received from any of the AI services it is stored so that calls to 'Current image property'
use the most recent response rather than ask for a new one.
This is because a project may need to access multiple pieces of a response.
</p>

<figure>
<div class="iframe-container" style="width: 800px; height: 350px;">
<iframe class="iframe-clipped"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/recognize new photo.xml"> 
</iframe></div>
<figcaption>A block for reporting the entire analysis response. TRY IT</figcaption>
</figure>

<p>The 'Recognize new photo' reporter block is implemented using the 'Ask &lt;provider&gt; to say what it sees' block.
This block does not wait for a response, instead it runs the user's blocks when a response is received.
It isn't as convenient 'Recognize new photo' but it can support more complex usage.
</p>

<figure>
<div class="iframe-container" style="width: 800px; height: 350px;">
<iframe class="iframe-clipped"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/recognize asynchronous.xml"> 
</iframe></div>
<figcaption>A block for obtaining the entire analysis response as a callback. TRY IT</figcaption>
</figure>

<h4 id="api-keys">How to provide API keys</h4>
<p>This chapter uses the API keys provided in the text areas above.
When constructing a project there are two ways to provide keys:
<ol>
<li>Add extra information to the page's URL.
Appending "&IBM Watson image key=...&Google image key=...&Microsoft image key=..." to a shared project URL will
provide all three keys (after "..." is replaced by the real keys).
Refresh the page after adding the keys.
If some service providers aren't being used then there is no need to include their keys in the URL.</li>
<li>Edit any of the following special reporters (found in the <a href="#library">eCraft2Learn block library</a>):</li>
</ol>

<figure>
<div class="iframe-container" style="width: 800px; height: 170px;">
<iframe class="iframe-clipped"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/blank keys.xml"> 
</iframe></div>
<figcaption>Reporters for API keys that users edit to provide service keys.</figcaption>
</figure>

<p>Because all the AI cloud services are commercial services heavy use can be costly.
Consequently it is best if each student has their own account and minimises the sharing of their keys.
This conflicts with one of the great things about tools like Scratch and Snap! --
that it is so easy to share one's projects with a wider community.
Adding the keys to the URL solves this problem if one is careful to keep the URL with keys private
and share the version without the keys.
It is possible but more awkward to maintain private and public versions when using reporters for keys.</p>

<h4 id="image-recognition-good-for">What is image recognition good for?</h4>
<p>This is like asking what is vision good for.
According to <a href="https://en.wikipedia.org/wiki/Evolution_of_the_eye" target="_blank">Wikipedia</a>
eyes have independently evolved between 50 and 100 times since animals first appeared.
Computer vision enables robots to see, self-driving cars (which really are just a kind of robot),
new kinds of user interfaces, support for doctors, police, sports coaches, farmers, the military, and more.
It can help blind people to navigate their surroundings and to use devices.
</p>

<h4 id="image-recognition-dangers">What are the dangers of image recognition?</h4>
<p>Like much of AI technology it may take away many jobs.
It may reduce our privacy since it makes it much easier to track people's movements and activities.
It can enable autonomous weapons that may kill many.</p>

<p>Another danger is that either intentionally or unintentially the data
the machine learning system is trained on contains biases.
Here is a nice short video on bias and machine learning published by Google.</p>
<iframe width="560" height="315"
        src="https://www.youtube.com/embed/59bMh59JQDo"
        frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

<h4 id="how-does-computer-vision-work">How does computer vision work?</h4>
<p>Image recognition begins with pixels.
Each pixel is a number or three numbers (for red, green, and blue components) that corresponds to a tiny piece of an image.
There are two main approaches to processing images:
<ol>
<li>A sequence of programmed processing steps to find edges, determine textures, identify objects, etc.</li>
<li>A machine learning system</li>
</ol>

Most of the recent progress has been with machine learning systems that for many tasks are near human level.
For some tasks such as
<a href="https://www.extremetech.com/extreme/233746-ai-beats-doctors-at-visual-diagnosis-observes-many-times-more-lung-cancer-signals" target="_blank">
detecting lung cancer from x-rays</a> machine learning has exceeded the ability of experts.</p>

<p>A neural net is a program that is inspired by what little is known about how neurons in animal and human brains work.
It can be trained to identify images.
In the most common case it is trained on thousands or millions of images that have already been labelled.
When given a new image and it computes the most likely labels.</p>

<p>Different levels or stages of the processing by the neural net recognise different elements of an image.
Researchers have built
<a href="https://distill.pub/2017/feature-visualization/" target="_blank">software that figures out what images produce the greatest response from the different layers</a>.
</p>

<figure>
<img src="images/edges layer.png" class="center">
<figcaption>Ideal images for a neural net layer that looks for edges</figcaption></figure>
<figure>
<img src="images/textures layer.png" class="center">
<figcaption>Ideal images for a neural net layer that looks for textures</figcaption></figure>
<figure>
<img src="images/patterns layer.png" class="center">
<figcaption>Ideal images for a neural net layer that looks for patterns</figcaption></figure>
<figure>
<img src="images/parts layer.png" class="center">
<figcaption>Ideal images for a neural net layer that looks for parts</figcaption></figure>
<figure>
<img src="images/objects layer.png" class="center">
<figcaption>Ideal images for a neural net layer that looks for objects</figcaption></figure>

<h4>What about video recognition services?</h4>
<p>Some AI cloud providers such as <a href="https://cloud.google.com/video-intelligence/" target="_blank">Google</a>
can accept a video stream and produce labels.
It can also detect scene changes.
This service can be expensive, though Google will analyse 1000 minutes of video per month at no cost.
There currently are no Snap! blocks for supporting video input.</p>

<h4 id="project-ideas">Possible project ideas using image recognition</h4>
<p>Adding image recognition to a robotics project can make the robot behave much more intelligently.
It can head towards goals and avoid specified objects.
A simple example would be a robot told to move to X (where X can be "the red ball", "the toy truck", "a person", or whatever).
It then sends an image for recognition.
If the description returned matches X then go forward a few steps (assuming the camera is mounted pointing forward).
If not then turn a little and try again.
Repeat until X is reached.
</p>
<p>For projects that use a camera that can't move there are many possibilities:
<ol>
<li>Engages in a simple conversation about what it thinks it is seeing.</li>
<li>Responds to what it sees.
E.g. says "How cute" when the description includes words such as "kitten" or "puppy" but says "How scary!" when the description is a toy lion or wolf.</li>
<li>Enhance the <a href="chapter-2.html#story-generator" target="_blank">story generator example</a>
so that in addition to tokens that ask for phrases or names a new kind of token is introduced that causes the current image to be recognised and
then substitutes the resulting description into that location of the story.</li>
<li>When a face is recognised its location and the location of parts of the face are included in the response. 
The app can then add know where to places glassses, mustaches, etc. to the image.</li>
<li>And thousands more possibilities.</li>
</ol>

<h3>Additional resources</h3>
<a href="https://cloud.google.com/vision/" target="_blank">Google</a>,
<a href="https://azure.microsoft.com/en-gb/services/cognitive-services/computer-vision/" target="_blank">Microsoft</a>,
and
<a href="https://www.ibm.com/watson/services/visual-recognition/" target="_blank">IBM Watson</a>
document their AI vision services.
There are many more vision services, see for example this
<a href="https://www.upwork.com/hiring/data/comparing-image-recognition-apis/" target="_blank">image recognition services comparison page</a>.
The <a href="https://en.wikipedia.org/wiki/Computer_vision" target="_blank">Wikipedia page</a> on computer vision is very thorough.
There are many MOOCS on computer vision and machine learning for image recognition.
<a href="https://distill.pub/" target="_blank">Distill</a> is a scientific journal that strives to explain clearly complex machine learning topics.
</p>

<script src="where-to-get-library.js"></script>

<h3>Learn about machine learning</h3>
<p>The <a href="chapter-4.html">next chapter</a> is about machine learning.</p>

<a href="chapter-2.html">Return to the previous chapter on speech input</a>.

<script src="acknowledgements.js"></script>

</body></html>