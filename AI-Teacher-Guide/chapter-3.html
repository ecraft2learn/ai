<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>Adding image recognition to programs</title>
<link href="../css/ai-teacher-guide.css" rel="stylesheet">
<link rel="icon" type="image/png" href="../images/eCraft2Learn-Favicon.png" />
<script src="../js/ai-guide.js"></script>
</head>
<body>
<script src="../js/translate.js"></script>
<h2>A guide to building AI apps and artefacts</h2>
<h3>Chapter 3 - Adding image recognition to programs</h3>
<h4>Ken Kahn, University of Oxford</h4>
<p class="resources">
You can export the blocks presented here as a
<a href="/ai/snap/snap.html?project=seeing&editMode" target="_blank">project</a> or 
download as a <a href="seeing blocks.xml" download target="_blank">library</a> to import into your projects.
</p>
<div id="table-of-contents"></div>
<h4 class="guide-to-guide-white">Browser compatibility</h4>
<p class="guide-to-guide">
This chapter of the guide includes many interactive elements that currently only run well in the Chrome browser.
This chapter relies upon there being a camera that the browser can access.
There is a <a href="troubleshooting.html" target="_blank">troubleshooting guide</a>
that should be consulted if problems are encountered.
</p>

<h4 class="background-information-white" id="introduction">Introduction</h4>
<p class="background-information">
A camera connected to a computer can report the colour of each pixel in an image and not much else.
A description of what is in front of the camera is returned when those pixels
are sent to an image recognition service.
There are many kinds of things an image description may contain.
With speech recognition the description is what was spoken, how confident the system is, and possible alternatives.
With image recognition there are many possible descriptions: descriptive tags, possible captions, dominant colours,
location of faces and parts of the face if there are any,
and the presence of landmarks, celebrities, well-known entities, and logos.
Hand-written and scanned text can also be recognised.
</p>

<p class="advanced-information non-essential">
A challenge in providing student-friendly programming blocks for image recognition is that
different AI cloud services report different descriptions in different ways.
We currently provide interfaces to image recognition services provided by Google and Microsoft.
A further challenge is how to provide simple interfaces for simple tasks
while still supporting more sophisticated uses and projects.
</p>

<p class="societal-impact">
In the last few years there has been tremendous progress in computer vision.
There are high performance systems for identifying objects, recognising faces, interpreting sketches,
and using medical images to aid diagnosis.
Driverless cars rely heavily upon computer vision.
</p>

<h4 id="api-keys" class="background-information-white">API keys are needed</h4>
<p class="background-information">
Chrome has built-in support for speech synthesis and recognition.
No browser, however, currently supports image recognition.
To access vision recognition services from companies such as Google or Microsoft
one needs to <a href="https://github.com/ecraft2learn../wiki" target="_blank">open an account</a>.
Accounts are free and provide some degree of free usage.
Microsoft permits 5000 queries per month, and Google 1000 per month.
To try the vision blocks described here you need at least one account.
Comparing and contrasting the results from different services are interesting ways
to gain some insight into how these services work.
</p>
<p class="background-information">
An alternative to using AI cloud services for image classification is
to load deep learning vision models into the browser.
Some of the blocks in the <a class="guide-link" href="chapter-4.html">next chapter</a>
can be used to analyse images.
Some can be further trained by users which is
especially useful when one wants to distinguish between images for idiosyncratic reasons (e.g. different facial expressions).
One has been trained to recognise one thousand different kinds of things (but not people).
While the models that run in the browser are not as capable or accurate as AI cloud services they have
the advantage that no data leaves your device, maintaining privacy.
Furthermore, they work even without a network connection.
</p>

<p class="background-information">
In projects there is a different way of providing keys to Snap!
(as described <a href="#api-keys-in-snap">here</a>) but in this chapter you can paste your key or keys below
and they'll be passed to the services as you use example blocks.
<br><label>Copy and paste your Google key here:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<input id="Google image key" type="text" style="width:24em;"></label>
<br><label>Copy and paste your Microsoft key here:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<input id="Microsoft image key" type="text" style="width:24em;"></label>

<h4 class="instructions-white" id="simple-image-recognition-block">A simple image recognition block</h4>
<p class="instructions">
This block takes a picture, sends it to the AI vision cloud server provider, waits for a response, and then
reports a list of labels of the photo.
The list is ordered by how confident the vision provider is that the label matches the image.
</p>

<figure class = "snap-iframe"
        id = "image labels reporter"
        stage_ratio = "0.25"
        container_style = "width: 800px; height: 350px" 
        caption = "A block for reporting the labels describing what the camera is showing. TRY IT">
</figure>

<h4 class="instructions-white" id="displaying-recognised-image">Displaying the image that was sent for recognition</h4>
<p class="instructions">
The <span class="block-name">show current photo</span> block will display
the most recent image sent to the specified AI vision services
as the background of the Snap! stage.
To try it first obtain some labels from a vision service.
You can also use the <span class="block-name">use camera to create costume</span> block
to take a new picture and add it as a costume to the current sprite.
</p>

<figure class = "snap-iframe"
        id = "image labels reporter and show photo"
        container_style = "width: 1000px; height: 550px" 
        iframe_style = "margin-left: -790px; margin-top: 150px;"
        caption = "After sending a photo for analysis you can use a block to display the image. TRY IT">
</figure>

<h4 class="sample-program-white" id="sample-image-recognition-project">A sample program using image recognition</h4>
<p class="sample-program">
Here is a program that can contact any of Google or Microsoft and displays the tags returned.
</p>

<figure class = "snap-iframe"
        id = "camera and say"
        full_screen = "true"
        container_style = "width: 800px; height: 600px" 
        caption = "Click on the AI cloud provider logos and see what happens">
</figure>

<h4 class="sample-program-white" id="image-and-speech-recognition">A sample program combining image and speech recognition and speech output</h4>
<p class="sample-program">
This program is similar to the previous one except it is listening for the words "Google"or "Microsoft".
When it hears one of these words it contacts that service.
When the response is received it turns it into speech.
As a demo it is impressive if you say things like "Tell me Google what do you see?" and "What do you see Microsoft?".
This will work in every language.
It is a nice illustration of how software can appear more intelligent than it is.
Do you think this program <i>seems</i> intelligent? If so, why? Is it intelligent?
</p>

<figure class = "snap-iframe"
        id = "camera, listen, and speak no keys"
        full_screen = "true"
        container_style = "width: 800px; height: 600px" 
        caption = 'Click the green flag and then say a sentence containing "Google" or "Microsoft"'>
</figure>

<span class="non-essential">
<h4 class="advanced-information-white" id="advanced-image-recognition">Advanced image recognition blocks</h4>
<p class="advanced-information">
Image recognisers can do more than label images.
Some can detect and locate faces.
Some of those will estimate the age of the person and their gender.
Some recognise landmarks and logos.
Many can recognise characters in hand-written or scanned text.
A problem creating Snap! blocks that provide this functionality is that
different services have different capabilities and different structures to capture their responses.
</p>

<h4 class="instructions-white" id="current-image-properies">Getting properties of an image</h4>
<p class="advanced-topic">Click to read an advanced topic</p>
<span class="advanced-topic-body">
<p class="instructions">
The <span class="block-name">Recognize new photo</span> block addresses this problem
by reporting a data structure capturing the entire response from
the vision service.
It is a Snap! list that contains lists that contains more data that might be text for labels,
numbers for confidence scores, and even more lists for complex data.
You can double click on the <img src="images/sub-list-icon.png" alt="icon for opening lists in lists">
icons to drill down the structure.
</p>
<p class="instructions">
The <span class="block-name">Current image property</span> block
takes an argument that describes what piece of the response structure
should be reported.
For example, Microsoft offers possible captions that can be found by following the path
<span class="block-name">"description captions text"</span>.
Common useful blocks are defined that use
the <span class="block-name">Current image property</span> block internally
to get the labels and the confidence scores of the labels for each of the supported vision service providers.
Note that after a response is received from any of the AI services it is stored so that calls to
<span class="block-name">Current image property</span> use the most recent response rather than ask for a new one.
This is because a project may need to access multiple pieces of a response.
</p>
<p class="instructions">
After having run <span class="block-name">Recognize new photo</span> block
you should select the same AI cloud provider
(currently Google or Microsoft)
when you run the <span class="block-name">Current image property</span> block.
The second argument can be a string or a list of strings specifying what information
is desired from the image recognition.
Each AI cloud provider supports different image properties:
<ul class="instructions">
<li>
<b>Google.</b> It supports properties that are lists of the form <span class="block-name">[labelAnnotations description]</span>,
<span class="block-name">[labelAnnotations score]</span> and <span class="block-name">[imagePropertiesAnnotation dominantColors colors]</span>.
More details
<a href="https://cloud.google.com/vision/docs/reference/rest/v1/images/annotate#AnnotateImageResponse" target="_blank">here</a>.
</li>
<li>
<b>Microsoft.</b> It supports properties that are lists of the form
<span class="block-name">[Categories name]</span>,
<span class="block-name">[Categories score]</span>,
<span class="block-name">[Description captions text]</span>,
<span class="block-name">[Description tags]</span>,
<span class="block-name">Faces</span>,
<span class="block-name">[Tags name]</span>, and
<span class="block-name">[Tags confidence]</span>.
More details
<a href="https://westcentralus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa" target="_blank">here</a>.
</li>
</ul>
</p>
<p class="instructions">
Common calls to the get property reporter are pre-defined.
They are 
<span class="block-name">Microsoft labels</span>, <span class="block-name">Microsoft first caption</span>,
<span class="block-name">Google labels</span>, and <span class="block-name">Google label scores</span>.
</p>
<p class="exercise">
<b>Exercise.</b>
Recognise a picture and then use
the <span class="block-name">Current image property</span> block to see
what kinds of descriptions the service provides.
How might each one be useful?
</p>

<p class="exercise"><b>Advanced Exercise.</b>
Compare the documentation for the vision services from
<a href="https://cloud.google.com/vision/" target="_blank">Google</a> and
<a href="https://azure.microsoft.com/en-gb/services/cognitive-services/computer-vision/" target="_blank">Microsoft</a>.
</p>

<figure class = "snap-iframe"
        id = "recognize new photo"
        container_style = "width: 800px; height: 330px" 
        caption = "A block for reporting the entire analysis response. TRY IT">
</figure>
</span>
</span>
<p class="advanced-topic">Click to read another advanced topic</p>
<span class="advanced-topic-body">
<p class="advanced-information">
The <span class="block-name">Recognize new photo</span> reporter block is implemented
using the <span class="block-name">Ask &lt;provider&gt; to say what it sees</span> block.
This block does not wait for a response, instead it runs the user's blocks when a response is received.
It isn't as convenient as 'Recognize new photo' but it can support more complex usage.
</p>

<figure class = "snap-iframe"
        id = "recognize asynchronous"
        container_style = "width: 800px; height: 350px" 
        caption = "A block for obtaining the entire analysis response as a callback. TRY IT">
</figure>
</span>
<h4 class="instructions-white" id="api-keys-in-snap">How to provide API keys</h4>
<p class="instructions">
This chapter uses the API keys provided in the text areas above.
When constructing a project there are two ways to provide keys:
<ol class="instructions">
<li>Add extra information to the page's URL.
Appending any (or all) of
<span class="block-name">&Google image key=...</span> or
<span class="block-name">&Microsoft image key=...</span>
to a shared project URL will
provide the keys (after "..." is replaced by the real keys).
You can provide only one key if you aren't interested in comparing the responses from different AI service providers.
Refresh the page after adding the keys.
</li>
<li>Edit one or both of the global variables
<span class="block-name">Google vision key</span> or
<span class="block-name">Microsoft vision key</span>.
Note these variables are declared as transient so they will not be set if you save and load your project.
</li>
</ol>

<p class="background-information">
Because all the AI cloud services are commercial services heavy use can be costly.
Consequently, it is best if each student has their own account and minimises the sharing of their keys.
This conflicts with one of the great things about tools like Scratch and Snap! --
that it is so easy to share one's projects with a wider community.
Adding the keys to the URL solves this problem if one is careful to keep the URL with keys private
and share the version without the keys.
</p>

<h4 class="societal-impact-white" id="image-recognition-good-for">What is image recognition good for?</h4>
<p class="societal-impact">
This is like asking what is vision good for.
According to <a href="https://en.wikipedia.org/wiki/Evolution_of_the_eye" target="_blank">Wikipedia</a>
eyes have independently evolved between 50 and 100 times since animals first appeared.
Computer vision enables robots to see, self-driving cars (which really are just a kind of robot),
new kinds of user interfaces, support for doctors, police, sports coaches, farmers, the military, and more.
It can help blind people to navigate their surroundings and to use devices.
</p>

<h4 class="societal-impact-white" id="image-recognition-dangers">What are the dangers of image recognition?</h4>
<p class="societal-impact">
Like much of AI technology it may take away many jobs.
It may reduce our privacy since it makes it much easier to track people's movements and activities.
It can enable autonomous weapons that may kill many.
</p>

<p class="societal-impact">
Another danger is that either intentionally or unintentionally the data
the machine learning system is trained on may contain biases.
Here is a nice short video on bias and machine learning published by Google.
</p>

<iframe class="societal-impact-white" 
        width="560" height="315"
        style="display: block; margin-left:auto; margin-right: auto;"
        src="https://www.youtube.com/embed/59bMh59JQDo"
        frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

<h4 class="how-it-works-white" id="how-does-computer-vision-work">How does computer vision work?</h4>
<p class="how-it-works">
Image recognition begins with pixels.
Each pixel is a number (for the gray level) or three numbers (for red, green, and blue components)
that corresponds to a tiny piece of an image.
There are two main approaches to processing images:
<br>
&nbsp;&nbsp;1. A sequence of programmed processing steps to find edges, determine textures, identify objects, etc.</li>
<br>
&nbsp;&nbsp;2. A machine learning system
</p>

<p class="societal-impact">
Most of the recent progress has been with machine learning systems that for many tasks are near or better than humans.
For some tasks such as
<a href="https://www.extremetech.com/extreme/233746-ai-beats-doctors-at-visual-diagnosis-observes-many-times-more-lung-cancer-signals" target="_blank">
detecting lung cancer from x-rays</a> machine learning has exceeded the ability of experts.
</p>

<p class="background-information">
A neural net is a program that is inspired by what little is known
about how neurons in animal and human brains work.
It can be trained to identify images.
In the most common case it is trained on thousands or millions of images that have already been labelled.
When given a new image and it computes the most likely labels.
Some easy visual discrimination tasks can be handled by training only a hundred or less images.
You can train a neural net to do this kind of vision recognition in the
<a class="guide-link" href="chapter-4.html">next chapter</a>.
</p>

<p class="background-information">
Different levels or stages of the processing by the neural net recognise different elements of an image.
Researchers have built
<a href="https://distill.pub/2017/feature-visualization/" target="_blank">
software that figures out what images produce the greatest response from the different layers</a>.
A very nice interactive tool for exploring what different pieces of a neural network respond to
is the <a href="https://microscope.openai.com" target="_blank">OpenAI Microscope</a>.
</p>

<figure class="background-information-white">
<img src="images/edges layer.png" class="center">
<figcaption>Ideal images for an early neural net layer that looks for edges</figcaption>
</figure>
<figure class="background-information-white">
<img src="images/textures layer.png" class="center">
<figcaption>Ideal images for a later neural net layer that looks for textures</figcaption>
</figure>
<figure class="background-information-white">
<img src="images/patterns layer.png" class="center">
<figcaption>Ideal images for a later neural net layer that looks for patterns</figcaption>
</figure>
<figure class="background-information-white">
<img src="images/parts layer.png" class="center">
<figcaption>Ideal images for a later neural net layer that looks for parts</figcaption>
</figure>
<figure class="background-information-white">
<img src="images/objects layer.png" class="center">
<figcaption>Ideal images for a late neural net layer that looks for objects</figcaption>
</figure>

<span class="non-essential">
<h4 class="background-information-white" id="video-recognition-services">What about <i>video</i> recognition services?</h4>
<p class="background-information">
Some AI cloud providers such as
<a href="https://cloud.google.com/video-intelligence/" target="_blank">Google</a>
can accept a video stream and produce labels.
It can also detect scene changes.
This service can be expensive, though Google will analyse 1000 minutes of video per month at no cost.
There currently are no Snap! blocks for supporting video input.
</p>
</span>

<h4 class="project-ideas-white" id="project-ideas">Possible project ideas using image recognition</h4>
<p class="project-ideas">
Adding image recognition to a robotics project can make the robot behave much more intelligently.
It can head towards goals and avoid specified objects.
A simple example would be a robot told to move to X
(where X can be "the red ball", "the toy truck", "a person", or whatever).
It begins by sending the current camera image for recognition.
If the description returned matches X then go forward a few steps
(assuming the camera is mounted pointing forward).
If not, then turn a little and try again.
Repeat until X is reached.
</p>
<p class="project-ideas">
For projects that use a camera that can't move there are many possibilities:
<ol class="project-ideas">
<li>Engage in a simple conversation about what it thinks it is seeing.</li>
<li>Respond to what it sees.
E.g. says "How cute" when the description includes words such as "kitten" or "puppy"
but says "How scary!" when the description is a toy lion or wolf.</li>
<li>Enhance the <a class="guide-link" href="chapter-2.html#story-generator" target="_blank">story generator example</a>
so that in addition to tokens that ask for phrases or names, a new kind of token
is introduced that causes the current image to be recognised and
then substitutes the resulting description into that location of the story.</li>
<li>When a face is recognised its location and the location of parts of the face are included in the response. 
The app can then add know where to place glasses, moustaches, etc. to the image.</li>
<li>And thousands more possibilities.</li>
</ol>

<h4 class="resources-white" id="additional-resources">Additional resources</h4>
<p class="advanced-topic">Click to read an advanced topic</p>
<span class="advanced-topic-body">
<p class="resources">
<a href="https://cloud.google.com/vision/" target="_blank">Google</a> and
<a href="https://azure.microsoft.com/en-gb/services/cognitive-services/computer-vision/" target="_blank">Microsoft</a>
document their AI vision services.
There are many more vision services, see for example this
<a href="https://www.upwork.com/hiring/data/comparing-image-recognition-apis/" target="_blank">image recognition services comparison page</a>.
The <a href="https://en.wikipedia.org/wiki/Computer_vision" target="_blank">Wikipedia page</a> on computer vision is very thorough.
There are many MOOCS on computer vision and machine learning for image recognition.
<a class="notranslate" translate=no href="https://distill.pub/" target="_blank">Distill</a> is a scientific journal that strives to explain clearly complex machine learning topics.
A <a href="https://distill.pub/2018/building-blocks/">Distill article</a> is an in-depth description of how a neural net does image recognition.
A <a href="https://www.nytimes.com/2018/03/06/technology/google-artificial-intelligence.html" target="_blank">general audience summary of the article</a> was written by the New York Times. 
Codecademy has a <a href="http://news.codecademy.com/what-is-machine-learning/" target="_blank">
good interview with a data scientist about machine learning</a>.
</p>
</span>

<h4 id="library" class="guide-to-guide-white">Where to get these blocks to use in your projects</h4>
<p class="resources">
You can export the blocks presented here as a
<a href="/ai/snap/snap.html?project=seeing&editMode" target="_blank">project</a> or 
download as a <a href="seeing blocks.xml" download target="_blank">library</a> to import into your projects.
</p>

<h4 class="guide-to-guide-white" id="machine-learning">Learn about machine learning</h4>
<p class="guide-to-guide">
The <a class="guide-link" href="chapter-4.html">next chapter</a>
is about machine learning.
</p>
<p class="guide-to-guide">
Return to
<a class="guide-link" href="chapter-2.html">the previous chapter on speech recognition</a>.
</p>

<script src="../js/bottom-of-page.js"></script>

</body>
</html>