<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>Machine Learning - AI Teacher's Guide</title>
<link href="ai-teacher-guide.css" rel="stylesheet">
<link rel="icon" type="image/png" href="images/eCraft2Learn-Favicon.png" />
</head>
<body>
<h2>A teacher's guide to helping students build AI apps and artefacts</h2>
<h3>Chapter 4 - Machine Learning</h3>
<h4>Ken Kahn, University of Oxford</h4>
<h3>Browser compatibility</h3>
<p>This chapter of the guide includes many interactive elements that currently only run well in the Chrome browser.
This chapter relies upon there being a camera that the browser can access.
Note that the blocks described here create a second browser tab for "training" the system.
The first time the "Train using camera ..." block is run you may see it blocked as a "popup".
See the <a href="troubleshooting.html" target="_blank">troubleshooting guide</a>
for how to handle this or if other problems are encountered.</p>
<h3>Introduction</h3>
<p>
In the previous chapters the new Snap! blocks used services that had been trained to
recognise speech and images.
In this chapter the students train the system.
This training is done by a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> system.
Deep learning is a very successful set of machine learning techniques.
Here is a very nice 5 minute video that introduces the big ideas behind machine learning:
</p>
<iframe width="560" height="315" 
        src="https://www.youtube.com/embed/_rdINNHLYaQ"
        frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
<p>
In this chapter we focus upon "supervised learning" which means that during training labels
for the input are provided.
The machine learns to then choose among training labels when presented with a new image (or other kind of input).
Rarely is the system completely confident that its choice of label is correct so instead it computes
"confidence scores".
The scores are between 0 (sure it is not this label) to 1 (sure).
The sum of the scores of all the labels always adds up to 1.
Students may prefer to display confidence scores as percentages.
For example, a .8 score for label X can be thought of as saying the system is
"80% sure the new input should have label X".
</p>
<p>
Unlike the previous chapters that rely upon pre-trained models here students can experiment with machine learning.
They can learn what kinds of inputs the system can easily tell apart and which ones confuse it.
Students can experiment to answer questions such as
how many examples of each label are needed to recognise them reliably and
how does recognition degrade as more labels are introduced.
</p>
<h4>A block to start training to recognise images from a camera</h4>
<p>
The following block opens a tab where you can train the system to label what is in front of the camera.
(If the tab is blocked by your browser read
the <a href="troubleshooting.html#train-camera" target="_blank">troubleshooting guide</a>.)
The first argument is the list of labels.
Here there are only two: "leaning left" and "leaning right".
You can edit the list to instead recognise different facial expressions, different people's faces,
objects held in front of the camera, and much more.
When the training tab launches you'll see two buttons:
</p>
<figure>
<img src="images/training-buttons.png" class="center">
<figcaption>Hold down each of these buttons as you present examples of leaning left or right</figcaption></figure>
<figure>
<div class="iframe-container"
     style="width: 800px; height: 350px;">
<iframe class="iframe-clipped"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/camera train.xml"> 
</iframe></div>
<figcaption>A block for training the system to recognise what the camera is showing. TRY IT</figcaption>
</figure>
<p>
When you are ready to test the trained system return to the Snap! tab
and click on the "Confidences of buckets" block.
Then click on the "confidences" variable to see its value.
Change what is in front of the camera and try again.
</p>
<h4>A sample program training and using image labels</h4>
<p>Here is a program that launches a training tab to learning
whether your finger is pointing at the camera or to the right.
When you return to Snap! the program will repeatedly check which way your finger is pointing.
If at the camera the turtle goes forward.
If not then it turns.
There are many ways of enhancing this program.
For example, add the label of pointing left and turn left or right depending on which way the finger is pointing.
</p>

<div>
<div class="iframe-container" style="width: 1000px; height: 400px;">
<figure>
<iframe class="iframe-clipped"
        style="margin-left: -500px; margin-top: 200px;"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/follow finger.xml"> 
</iframe></div>
<figcaption>Train it to learn which way your finger is pointing and the watch the turtle follow your finger</figcaption>
</figure>
</body>
</html>