<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>Adding machine learning to programs</title>
<link href="/ai/css/ai-teacher-guide.css" rel="stylesheet">
<link href="/ai/css/oer-style.css" rel="stylesheet">
<link rel="icon" type="image/png" href="/ai/images/eCraft2Learn-Favicon.png" />
<script src="/ai/js/ai-guide.js"></script>
</head>
<body>
<script src="/ai/js/translate.js"></script>
<h2>A guide to building AI apps and artefacts</h2>
<h3>Chapter 4 - Adding machine learning to programs</h3>
<h4>Ken Kahn, University of Oxford</h4>
<h3 id="browser-compatibility">Browser compatibility</h3>
<p>This chapter of the guide includes many interactive elements that currently only run well in the Chrome browser.
This chapter relies upon there being a camera that the browser can access.
Note that the blocks described here create a second browser tab for "training" the system.
The first time the <span class="block-name">Train with image buckets ...</span> block
is run you may see it blocked as a "popup".
See the <a href="troubleshooting.html" target="_blank">troubleshooting guide</a>
for how to handle this or if other problems are encountered.</p>
<h3>Introduction</h3>
<p>
In the previous chapters the new Snap! blocks used services that had been trained to
recognise speech and images.
<span class="teacher-guide">In this chapter the students train the system.</span>
<span class="student-guide">In this chapter you train the system.</span>
This training is done by a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> system.
Deep learning is a very successful set of machine learning techniques.
It called "deep" because there are many layers of simulated neurons that do the processing.
Typically the lowest layers pay attention to tiny details
but higher layers can use the output of lower layers that capture some aspect of the image such as texture or edges.
Here is a very nice 5 minute video that introduces the big ideas behind machine learning:
</p>
<iframe width="560" height="315" 
        src="https://www.youtube.com/embed/_rdINNHLYaQ"
        frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
<p>
In this chapter we focus upon "supervised learning" which means that during training labels
for the input are provided.
The machine learns to then choose among training labels when presented with a new image (or other kind of input).
Rarely is the system completely confident that its choice of label is correct so instead it computes
"confidence scores".
The scores are between 0 (certain it is not this label) to 1 (certain it is).
The sum of the scores of all the labels always adds up to 1.
<span class="teacher-guide">Students may prefer to display confidence scores as percentages.</span>
<span class="student-guide">You may prefer to display confidence scores as percentages by multiplying the scores by 100.</span>
For example, a .8 score for label X can be thought of as saying the system is
"80% sure the new input should have label X".
</p>
<p class="teacher-guide">
Unlike the previous chapters that rely upon pre-trained models here students can experiment with machine learning.
They can learn what kinds of inputs the system can easily tell apart and which ones confuse it.
Students can experiment to answer questions such as
how many examples of each label are needed to recognise them reliably and
how does recognition degrade as more labels are introduced.
</p>
<p class="student-guide">
Unlike the previous chapters that rely upon pre-trained models here you can experiment with machine learning.
You can learn what kinds of inputs the system can easily tell apart and which ones confuse it.
You can experiment to answer questions such as
how many examples of each label are needed to recognise them reliably and
how does recognition degrade as more labels are introduced.
</p>
<h4>A block to start training to recognise images from a camera</h4>
<p>
The <span class="block-name">Train with image buckets ...</span> block
opens a tab where you can train the system to label what is in front of the camera.
(If the tab is blocked by your browser read
the <a href="troubleshooting.html#train-camera" target="_blank">troubleshooting guide</a>.)
The first argument is the list of labels.
A <span class="notranslate" translate=no>"bucket"</span> is the collection of all the images with the same label.
The system will attempt to determine the confidence that an image matches the images in each bucket.
If you are training it to distinguish between two things (e.g. cats and dogs) then it is a good idea
to add a third bucket for "everything else".
During training then produce images of lots of different things that aren't cats or dogs while pressing the
button to train for the "everything else" bucket.
</p>
<p>
Here there are only two labels: "leaning to the left" and "leaning to the right".
You can edit the list to instead recognise different facial expressions, different people's faces,
objects held in front of the camera, and much more.
When the training tab launches you'll see two buttons:
</p>
<figure>
<img src="images/training-buttons.png" class="center">
<figcaption>Hold down each of these buttons as you present examples of leaning to the left or right</figcaption></figure>

<figure class = "snap-iframe"
        id = "camera train"
        container_style = "width: 800px; height: 350px" 
        caption = "A block for training the system to recognise what the camera is showing. TRY IT">
</figure>

<b class="student-guide advanced-topic">Click to read an advanced topic</b>
<span>
<p>
The <span class="block-name">Train with image buckets ...</span> block only needs the first argument telling it the bucket labels.
It has three additional optional arguments:
<ol>
<li>
Whether to start fresh or continue with the current training if this block is run more than once in a session.
</li>
<li>
Text that will appear on the training page. Can be in HTML format.
</li>
<li>
Blocks to run after returning from the training tab.
</li>
</ol>
</p>
</span>

<p>
When training the system try to provide a variety of images that match the label.
If you are training "leaning left" then lean different amounts and at different distances to the camera.
Then when training "leaning right" don't allow the system to learn some difference that shouldn't matter.
For example, don't change the lighting or background between training different labels.
When you aren't pressing any training buttons the training page keeps
trying to recognise what is in front of the camera.
You can determine if your training is good by leaning one way and then the other
and see if the confidence scores changes as they should.
If not try adding more images.
</p>
<p>
When you are ready to test the trained system return to the Snap! tab
and click on the <span class="block-name">Current image label confidences</span> block.
Then click on the <span class="block-name">confidences</span> variable to see its value.
Change what is in front of the camera and try again.
Note that when you return to the training tab it starts running and you can add more training samples.
When you are finished with this close the training tab.
</p>
<h4>A sample program training and using image labels</h4>
<p>Here is a program that launches a training tab to determine
whether your finger is pointing at the camera or to the right.
While training move your hand around so it is trained to ignore where the hand is
and just pay attention to which way the finger is pointing.
When you return to Snap! the program will repeatedly check which way your finger is pointing.
If it sees a finger pointing at the camera then the turtle goes forward.
If not then it turns.
It always displays the latest confidence scores.
</p>
<p>
<b>Exercise.</b>
There are many ways of enhancing this program.
Add a third or fourth label and use that label to control the turtle.
For example, add the label of pointing left and change the program to turn left or right depending on which way the finger is pointing.
Or add a gesture to stop moving.
</p>
<p>
A full screen version of this program can be
<a href="https://snap.berkeley.edu/snapsource/snap.html#present:Username=toontalk&ProjectName=follow%20finger" target="_blank">found here</a>.
There is a similar <a href="https://snap.berkeley.edu/snapsource/snap.html#present:Username=toontalk&ProjectName=follow%20lean" target="_blank">
program where the sprite moves left or right depending upon which way you are leaning</a>.
</p>

<figure class = "snap-iframe"
        id = "follow finger"
        container_style = "width: 1000px; height: 400px"
        iframe_style = "margin-left: -1000px; margin-top: 200px;"
        caption = "Train it to learn which way your finger is pointing and the watch the turtle follow your finger. TRY IT!">
</figure>

<p>
<b>Exercise.</b> Explore how to make the training more accurate.
How does the performance change as more examples are presented?
Does it help to simplify the images (e.g. arrange so only your pointing hand is being captured by the camera)?
If you train with one person and then replace with another does the system get confused?
Does it help to train with both people?
</p>

<h4>Using sprite costumes instead of the camera in training</h4>
<p>
In Snap! each sprite can have any number of "costumes".
Costumes can be drawings, photos, or imported images.
</p>
<figure>
<img src="images/snap-costume.png" class="center">
<figcaption>Snap! interface for creating costumes</figcaption></figure>

<p>
Using the <span class="block-name">Train with all my costumes</span> you can send all of a
sprite's costumes to the training tab.
This block is defined using the <span class="block-name">Add costume &lt;i&gt; to training ...</span> block.
You can also ask the training tab about label confidences of a costume using 
<span class="block-name">Label confidences of costume number ...</span>.
You can experiment with these costume-based training blocks in the following.
There is a
<a href="https://snap.berkeley.edu/snapsource/snap.html#present:Username=aiteacherguide&ProjectName=black%20and%20white"
target="_blank">full window version</a> if you want to explore the details of how this example works.
</p>

<figure class = "snap-iframe"
        id = "black and white"
        container_style = "width: 1000px; height: 550px;"
        iframe_style = "margin-left: 0px; margin-top: 0px;"
        caption = "Blocks for using sprite costumes for training and classification. TRY IT!">
</figure>

<h4>Benefits and risks</h4>
<p>
Most of the <a class="guide-link" href="chapter-3.html#image-recognition-good-for" target="_blank">things image recognition is good for</a>
and <a class="guide-link" href="chapter-3.html#image-recognition-dangers" target="_blank">its dangers</a> were discussed in chapter 3.
<span class="teacher-guide">Students may want to discuss whether there are benefits and risks when they do the training
instead of professionals in large corporations?</span>
<span class="student-guide">But maybe the situation is different
if users do the training instead of professionals in large corporations.</span>
Will the biases be different?
Does the smaller training set make the system less reliable?
</p>
<b class="student-guide advanced-topic">Click to read an advanced topic</b>
<span>
<h4>How does this work?</h4>
<p>
We discussed <a class="guide-link" href="chapter-3.html#how-does-computer-vision-work" target="_blank">how image recognition works in general</a>
in the previous chapter.
The fact that training of a deep learning system is feasible inside of browser is due to the work on
<a href="https://deeplearnjs.org" target="_blank">deeplearn.js</a>.
This JavaScript library speeds up learning and prediction by using the 
<a href="http://www.nvidia.com/object/what-is-gpu-computing.html" target="_blank">GPU</a>
that is part of nearly all computers.
The GPU can be more than one hundred times faster at the computations that deep learning needs than using
the CPU of the computer.
The training window is based upon the <a href="https://github.com/googlecreativelab/teachable-machine-boilerplate"
target="_blank">teachable machine boilerplate</a>.
The deep learning model used is called <a href="https://arxiv.org/abs/1602.07360" target="_blank">SqueezeNet</a>
because it is designed to be smaller and faster than more accurate architectures.
</p>
</span>

<h4>A Rock Paper Scissors game using Machine Learning</h4>
<p>
Using machine learning one can make a rock paper scissors game that can be played
by putting your hand into one of three configurations.
If the game below doesn't work visit this <a href="#browser-compatibility">help</a>.
</p>

<figure class = "snap-iframe"
        id = "rock paper scissors"
        full_screen = "true"
        container_style = "width: 800px; height: 600px" 
        caption = "Click the green flag to play Rock Paper Scissors using just your hand. TRY IT!">
</figure>

<p>This game uses the <span class="block-name">train with all my costumes ...</span> block four times.
Once for rocks, paper, and scissors and a fourth time for "other".
Once the training is completed the game starts and the program picks a random move
and compares it to what it thinks your move was based upon the analysis by the training tab
of what is in front of the camera.
You can explore and enhance the game using the
<a href="https://snap.berkeley.edu/snapsource/snap.html#present:Username=toontalk&ProjectName=rock%20paper%20scissors" target="_blank">
full window version</a>.
This example was inspired by the great set of machine learning project examples at
<a href="https://machinelearningforkids.co.uk/#!/worksheets" target="_blank">machine learning for kids</a>
</p>

<h4>Doesn't machine learning require huge amounts of data?</h4>
<p>
Much of the current success of machine learning system is a result of having trained them on <i>huge</i> data sets.
Deep learning for machine translation, for example, didn't work well until they were trained with billions of sentences.
So how could the examples in this chapter work with just dozens of examples for each label?
Part of the answer is that big data leads to better accuracy.
If you try the rock paper scissors game you'll see it sometimes makes mistakes.
The main difference however is that the image recognition services in the 
<a href="chapter-3.html" target="_blank">previous chapter</a> can distinguish between at least one thousand different kinds of images.
Rock paper scissors requires only four categories.
And AI vision services can do more than just
<a href="chapter-3.html#advanced-image-recognition" target="_blank">choose a label</a> for an image.
</p>

<h4>Recognising sounds</h4>
<p>
Computers can be trained to distinguish different sounds including spoken speech.
<a href="chapter-2.html">Chapter 3</a> describes how to use cloud services that do speech recognition.
Here you can create programs that respond to any sound.
Also you can train it recognise a few different words or phrases that works locally on your computer without sending
any audio to a cloud server.
This is useful in contexts without a reliable network connection or for increased privacy.
</p>
<figure class = "snap-iframe"
        id = "train audio"
        full_screen = "false"
        stage_ratio = ".01"
        container_style = "width: 825px; height: 625px" 
        caption = "A block for training the system to recognise what the microphone is hearing. TRY IT">
</figure>

<p>
The <span class="block-name">Train with audio buckets...</span> block currently uses a simple learning algorithm.
The plan is to replace it with a deep learning model.
</p>
<h4>An Example Program using the Microphone and Machine Learning</h4>
<p>
Using machine learning one can make a rock paper scissors game that can be played
by putting your hand into one of three configurations.
If the game below doesn't work visit this <a href="#browser-compatibility">help</a>.
</p>

<figure class = "snap-iframe"
        id = "train and speak commands"
        full_screen = "true"
        container_style = "width: 800px; height: 600px" 
        caption = "Click the green flag to train it to recognise a few voice commands. TRY IT">
</figure>

<h4 id="project-ideas">Possible project ideas using image recognition</h4>
<p>
Most of the <a class="guide-link" href="chapter-3.html#project-ideas" target="_blank">project ideas listed for using image recognition services</a>
can be based upon the blocks described in this chapter.
There is much that training by users enables that cloud services can't do.
You can train the system to recognise yourself, distinguish between your friends,
gestures, and the things that
your robots may encounter in its particular environment.
Here are some project ideas that exploit the ability of users to train the system:
<ul>
<li>
A different rock, paper, scissors app would be one that is trained watching two human players.
It can then keep score as people play the game.
</li>
<li>
Yet another different rock, paper, scissors app could learn patterns in how a human
player plays and predict what they will do next with better than 1/3 accuracy.
</li>
<li>
A scientific classification app.
It could be trained to tell the name of a plant when shown one of its leaves,
or identify insects, rocks, etc.
</li>
<li>
Identifying different sounds. Train the system to identify what muscial intstrument is being played.
Or which bird is singing.
</li>
<li>
A robot that recognises gestures such as "stop" when it sees an outstretched palm.
</li>
<li>
Control a game element with four hand positions for left, right, up, and down.
Maybe left and right could be pointing left or right, up could be thumbs up, and down thumbs down.
Create your own game, modify one already in Snap!, or <a href="https://djdolphin.github.io/Snapin8r2/" target="_blank">import a Scratch game</a>.
This idea was inspired by this
<a href="https://medium.com/@mariannelinharesm/playing-a-game-using-just-your-camera-with-deeplearnjs-ca156008f537" target="_blank">
article about using hand gestures to play Tetris, Snake, and Asteroids</a>.
</li>
<li>
A program that greets fellow students by name when they sit in front of the camera.
</li>
<li>
A safety app that can tell if someone is lying on the floor or not.
If someone is down for <i>x</i> minutes it contacts help.
Or an app that can tell if a bed is empty or not and takes some action if the bed is occupied well after
the time the occupant usually gets up.
</li>
<li>A sorting robot that is trained to distinguish a small number of different kinds of things.
It then moves the items into piles for each kind of thing.
</li>
<li>
Replace the mouse with four head movements: up, down, left, and right. 
What could be used for "mouse click"?
Who might want to use such a system?
</li>
<li>
Train the computer to know which part of a face is being pointed to.
Have the program do different things depending upon whether you point to your eyes, ears, or mouth.
</li>
<li>
And thousands more. Be creative.
</li>
</ul>
The <a href="https://machinelearningforkids.co.uk/#!/worksheets" target="_blank">machine learning for kids site</a>
also has sample projects.
</p>
<h4>Future directions for this chapter</h4>
<p>
Several enhancements to this chapter are planned.
These include support for other kinds of input such as text or numbers,
blocks for saving and restoring a trained model, and more.
</p>

<h3 id="additional-resources">Additional resources</h3>
<p>
In addition to the <a href="chapter-3.html#additional-resources" target="_blank">resources listed in the previous chapter</a>
there is the <a href="https://teachablemachine.withgoogle.com/" target="_blank">teachable machine site</a>.
Technical details on the deep learning model used here are <a href="https://github.com/googlecreativelab/teachable-machine-boilerplate"
target="_blank">documented here</a>.
</p>
<p>
Return to 
<a class="guide-link" href="chapter-3.html">the previous chapter on using image recognition services</a>.
</p>
<script src="/ai/js/bottom-of-page.js"></script>
</body>
</html>