<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>Machine Learning - AI Teacher's Guide</title>
<link href="ai-teacher-guide.css" rel="stylesheet">
<link rel="icon" type="image/png" href="images/eCraft2Learn-Favicon.png" />
</head>
<body>
<h2>A teacher's guide to helping students build AI apps and artefacts</h2>
<h3>Chapter 4 - Machine Learning</h3>
<h4>Ken Kahn, University of Oxford</h4>
<h3>Browser compatibility</h3>
<p>This chapter of the guide includes many interactive elements that currently only run well in the Chrome browser.
This chapter relies upon there being a camera that the browser can access.
Note that the blocks described here create a second browser tab for "training" the system.
The first time the "Train using camera ..." block is run you may see it blocked as a "popup".
See the <a href="troubleshooting.html" target="_blank">troubleshooting guide</a>
for how to handle this or if other problems are encountered.</p>
<h3>Introduction</h3>
<p>
In the previous chapters the new Snap! blocks used services that had been trained to
recognise speech and images.
In this chapter the students train the system.
This training is done by a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> system.
Deep learning is a very successful set of machine learning techniques.
Here is a very nice 5 minute video that introduces the big ideas behind machine learning:
</p>
<iframe width="560" height="315" 
        src="https://www.youtube.com/embed/_rdINNHLYaQ"
        frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
<p>
In this chapter we focus upon "supervised learning" which means that during training labels
for the input are provided.
The machine learns to then choose among training labels when presented with a new image (or other kind of input).
Rarely is the system completely confident that its choice of label is correct so instead it computes
"confidence scores".
The scores are between 0 (certain it is not this label) to 1 (certain it is).
The sum of the scores of all the labels always adds up to 1.
Students may prefer to display confidence scores as percentages.
For example, a .8 score for label X can be thought of as saying the system is
"80% sure the new input should have label X".
</p>
<p>
Unlike the previous chapters that rely upon pre-trained models here students can experiment with machine learning.
They can learn what kinds of inputs the system can easily tell apart and which ones confuse it.
Students can experiment to answer questions such as
how many examples of each label are needed to recognise them reliably and
how does recognition degrade as more labels are introduced.
</p>
<h4>A block to start training to recognise images from a camera</h4>
<p>
The "train using camera" block opens a tab where you can train the system to label what is in front of the camera.
(If the tab is blocked by your browser read
the <a href="troubleshooting.html#train-camera" target="_blank">troubleshooting guide</a>.)
The first argument is the list of labels.
A "bucket" is the collection of all the images with the same label.
Here there are only two: "leaning left" and "leaning right".
You can edit the list to instead recognise different facial expressions, different people's faces,
objects held in front of the camera, and much more.
When the training tab launches you'll see two buttons:
</p>
<figure>
<img src="images/training-buttons.png" class="center">
<figcaption>Hold down each of these buttons as you present examples of leaning left or right</figcaption></figure>
<figure>
<div class="iframe-container"
     style="width: 800px; height: 350px;">
<iframe class="iframe-clipped"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/camera train.xml"> 
</iframe></div>
<figcaption>A block for training the system to recognise what the camera is showing. TRY IT</figcaption>
</figure>
<p>
When you are ready to test the trained system return to the Snap! tab
and click on the "Confidences of buckets" block.
Then click on the "confidences" variable to see its value.
Change what is in front of the camera and try again.
Note that when you return to the training tab it starts running and you can add more training samples.
</p>
<h4>A sample program training and using image labels</h4>
<p>Here is a program that launches a training tab to learning
whether your finger is pointing at the camera or to the right.
While training move your hand around so it is trained to ignore where the hand is
and just pay attention to which way the finger is pointing.
When you return to Snap! the program will repeatedly check which way your finger is pointing.
If it sees a finger pointing at the camera then the turtle goes forward.
If not then it turns.
There are many ways of enhancing this program.
For example, add the label of pointing left and turn left or right depending on which way the finger is pointing.
A full screen version of this program can be
<a href="https://snap.berkeley.edu/snapsource/snap.html#present:Username=toontalk&ProjectName=follow%20finger" href="_blank">found here</a>.
There is a similar <a href="https://snap.berkeley.edu/snapsource/snap.html#present:Username=toontalk&ProjectName=follow%20lean" href="_blank">
program where the sprite moves left or right depending upon which way you are leaning</a>.
</p>

<div>
<div class="iframe-container" style="width: 1000px; height: 400px;">
<figure>
<iframe class="iframe-clipped"
        style="margin-left: -1000px; margin-top: 200px;"
        scrolling="no"
        src="../snap/snap.html"
        project_path="../AI-teacher-guide-projects/follow finger.xml"> 
</iframe></div>
<figcaption>Train it to learn which way your finger is pointing and the watch the turtle follow your finger</figcaption>
</figure>
<h4>Benefits and risks</h4>
<p>
Most of the <a href="chapter-3.html#image-recognition-good-for" target="_blank">things image recognition is good for</a>
and <a href="chapter-3.html#image-recognition-dangers" target="_blank">its dangers</a> were discussed in chapter 3.
Students may want to discuss whether there are benefits and risks when they do the training
instead of professionals in large corporations?
Will the biases be different?
Does the smaller training set make the system less reliable?
</p>
<h4>How does this work?</h4>
<p>
We discussed <a href="chapter-3.html#how-does-computer-vision-work" target="_blank">how image recognition works in general</a>
in the previous chapter.
The fact that training of a deep learning system is feasible inside of browser is due to the work on
<a href="https://deeplearnjs.org" target="_blank">deeplearn.js</a>.
This JavaScript library speeds up learning and prediction by using the 
<a href="http://www.nvidia.com/object/what-is-gpu-computing.html" target="_blank">GPU</a>
that is part of nearly all computers.
The GPU can be more than one hundred times faster at the computations that deep learning needs than using
the CPU of the computer.
The training window is based upon the <a href="https://github.com/googlecreativelab/teachable-machine-boilerplate"
target="_blank">teachable machine boilerplate</a>.
</p>
<h4 id="project-ideas">Possible project ideas using image recognition</h4>
<p>
Most of the <a href="chapter-3.html#project-ideas" target="_blank">project ideas listed for using image recognition services</a>
can be based upon the blocks described in this chapter.
There is much that training by students enables that cloud services can't do.
Students can train the system to recognise themselves, their fellow students, gestures, and the things that
their robots may encounter in its particular environment.
Here are some project ideas that exploit the ability of students to train the system:
<ul>
<li>
Rock, paper, and scissors. The computer can be trained to tell when someone's hand
is showing "rock", "paper", or "scissors".
A game can be created where one plays the game by using your hand just as one does when playing with humans.
(Note that one should probably not program the computer to cheat and make its choice after seeing what the player has done.)
This example was inspired by the great set of machine learning project examples at
<a href="https://machinelearningforkids.co.uk/#!/worksheets" target="_blank">machine learning for kids</a>.
</li>
<li>
A scientific classification app. It could be trained to tell the name of a plant when shown one of its leaves,
or identify insects, rocks, etc.
</li>
<li>
A robot that recognises gestures such as "stop" when it sees an outstretched palm.
</li>
<li>
Control a game element with four hand positions for left, right, up, and down.
Maybe left and right could be pointing left or right, up could be thumbs up, and down thumbs down.
Create your own game, modify one already in Snap!, or <a href="https://djdolphin.github.io/Snapin8r2/" target+"_blank">import a Scratch game</a>.
This idea was inspired by this <a href="https://medium.com/@mariannelinharesm/playing-a-game-using-just-your-camera-with-deeplearnjs-ca156008f537" target="_blank">
article about using hand gestures to play tetris, snake, and asteroids</a>.
</li>
<li>
A program that greets fellow students by name when they sit in front of the camera.
</li>
<li>
A safety app that can tell if someone is lying on the floor or not.
If so for <i>x</i> minutes it contacts help.
Or an app that can tell if a bed is empty or not and takes some action if the bed is occupied well after
the time the occupant usually gets up.
</li>
<li>A sorting robot that is trained to distinguish a small number of different kinds of things.
It then moves the items into piles for each kind of thing.
</li>
<li>
And thousands more. Be creative.
</li>
</ul>
The <a href="https://machinelearningforkids.co.uk/#!/worksheets" target="_blank">machine learning for kids site</a>
also has sample projects.
</p>
<h4>Future directions for this chapter</h4>
<p>
Several enhancements to this chapter are planned.
These include support for other kinds of input such as audio or text,
a block for adding new training data,
blocks for saving and restoring a trained model,
an option when training to use image files instead of the camera,
and more.
</p>

<h3 id="additional-resources">Additional resources</h3>
<p>
In addition to the <a href="chapter-3.html#additional-resources" target="_blank">resources listed in the previous chapter</a>
there is the <a href="https://teachablemachine.withgoogle.com/" target="_blank">teachable machine site</a>.
Technical details on the deep learning model used here are <a href="https://github.com/googlecreativelab/teachable-machine-boilerplate"
target="_blank">documented here</a>.
</p>
<a href="chapter-3.html">Return to the previous chapter on using image recognition services</a>.
</body>
</html>