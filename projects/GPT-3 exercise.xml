<project name="GPT-3 exercise" app="Snap! 7, https://snap.berkeley.edu" version="2"><notes></notes><thumbnail>data:image/png;base64,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</thumbnail><scenes select="1"><scene name="GPT-3 exercise"><notes></notes><palette><category name="GPT-3 Blocks" color="199,0,164,1"/><category name="Jurassic 1" color="127,0,235,1"/></palette><hidden></hidden><headers></headers><code></code><blocks><block-definition s="temperature %&apos;temperature&apos;" type="reporter" category="GPT-3 Blocks"><comment x="0" y="0" w="298.5714285714287" collapsed="false">What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.&#xD;&#xD;We generally recommend altering this or top_p but not both.</comment><header></header><code></code><translations></translations><inputs><input type="%n"></input></inputs><script><block s="doReport"><block s="reportJoinWords"><list><l>"temperature": </l><block var="temperature"/></list></block></block></script></block-definition><block-definition s="maximum number of tokens %&apos;max&apos;" type="reporter" category="GPT-3 Blocks"><comment x="0" y="0" w="337.85714285714295" collapsed="false">The maximum number of tokens to generate in the completion.&#xD;&#xD;The token count of your prompt plus max_tokens cannot exceed the model&apos;s context length. Most models have a context length of 2048 tokens.</comment><header></header><code></code><translations></translations><inputs><input type="%n">16</input></inputs><script><block s="doReport"><block s="reportJoinWords"><list><l>"max_tokens": </l><block var="max"/></list></block></block></script></block-definition><block-definition s="top p %&apos;top p&apos;" type="reporter" category="GPT-3 Blocks"><comment x="0" y="0" w="267.8571428571429" collapsed="false">An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.&#xD;&#xD;We generally recommend altering this or temperature but not both.</comment><header></header><code></code><translations></translations><inputs><input type="%n">1</input></inputs><script><block s="doReport"><block s="reportJoinWords"><list><l>"top_p": </l><block var="top p"/></list></block></block></script></block-definition><block-definition s="presence penalty %&apos;penalty&apos;" type="reporter" category="GPT-3 Blocks"><comment x="0" y="0" w="420" collapsed="false">Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&apos;s likelihood to talk about new topics.&#xD;More info at&#xD;https://beta.openai.com/docs/api-reference/parameter-details</comment><header></header><code></code><translations></translations><inputs><input type="%n">0</input></inputs><script><block s="doReport"><block s="reportJoinWords"><list><l>"presence_penalty": </l><block var="penalty"/></list></block></block></script></block-definition><block-definition s="frequency penalty %&apos;penalty&apos;" type="reporter" category="GPT-3 Blocks"><comment x="0" y="0" w="304.853515625" collapsed="false">Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&apos;s likelihood to repeat the same line verbatim.&#xD;&#xD;More info at&#xD;https://beta.openai.com/docs/api-reference/parameter-details</comment><header></header><code></code><translations></translations><inputs><input type="%n">0</input></inputs><script><block s="doReport"><block s="reportJoinWords"><list><l>"frequency_penalty": </l><block var="penalty"/></list></block></block></script></block-definition><block-definition s="stop completing before generating any of %&apos;stop&apos;" type="reporter" category="GPT-3 Blocks"><header></header><code></code><translations></translations><inputs><input type="%mult%txt"></input></inputs><script><block s="doIfElse"><block s="reportEquals"><block s="reportListAttribute"><l><option>length</option></l><block var="stop"/></block><l>1</l></block><script><block s="doReport"><block s="reportJoinWords"><list><l>"stop": "</l><block var="stop"/><l>"</l></list></block></block></script><script><block s="doReport"><block s="reportJoinWords"><list><l>"stop": [</l><block s="reportMap"><block s="reifyReporter"><autolambda><block s="reportJoinWords"><list><l>"</l><l></l><l>",</l></list></block></autolambda><list></list></block><custom-block s="all but last %l"><block var="stop"/></custom-block></block><block s="reportJoinWords"><list><l>"</l><block s="reportListItem"><l><option>last</option></l><block var="stop"/></block><l>"</l></list></block><l>]</l></list></block></block></script></block></script></block-definition><block-definition s="complete %&apos;prompt&apos; using Jurassic 1 engine named %&apos;engine&apos; with key %&apos;api key&apos; $nl with options %&apos;options&apos;" type="reporter" category="Jurassic 1"><comment x="0" y="0" w="398.5714285714288" collapsed="false">Ask the specified Jurassic 1 engine to complete the prompt with the options provided.&#xD;&#xD;Input 3 must be your OpenAI API key which you can obtain from ai21.com&#xD;For documentation on the options see https://studio.ai21.com/docs/api/</comment><header></header><code></code><translations></translations><inputs><input type="%txt"></input><input type="%txt">j1-jumbo<options>j1-jumbo&#xD;j1-large</options></input><input type="%txt"></input><input type="%mult%txt"></input></inputs><script><custom-block s="let %upvar be %s"><l>message</l><block s="reportJoinWords"><list><l>{</l><block s="reportMap"><block s="reifyReporter"><autolambda><block s="reportIfElse"><block s="reportEquals"><block s="reportStringSize"><l></l></block><l>0</l></block><l></l><block s="reportJoinWords"><list><l></l><l>,</l></list></block><comment w="126.42857142857143" collapsed="true">Ignore empty options.</comment></block></autolambda><list></list></block><block var="options"/></block><custom-block s="prompt %txt"><custom-block s="sanitize prompt %txt"><block var="prompt"/></custom-block></custom-block><l>}</l></list></block></custom-block><custom-block s="let %upvar be %s"><l>response string</l><custom-block s="%s url: %s send: %s headers: %mult%l"><l>POST</l><block s="reportJoinWords"><list><l>https://api.ai21.com/studio/v1/</l><block var="engine"/><l>/complete</l></list></block><block var="message"/><list><custom-block s="key: %s value: %s"><l>Authorization</l><block s="reportJoinWords"><list><l>Bearer </l><block var="api key"/></list></block></custom-block><custom-block s="key: %s value: %s"><l>Content-Type</l><l>application/json</l></custom-block></list></custom-block></custom-block><custom-block s="let %upvar be %s"><l>response</l><block s="reportTextSplit"><block var="response string"/><l><option>json</option></l></block></custom-block><block s="doIfElse"><block s="reportIsA"><custom-block s="$flash assoc %s %l"><l>completions</l><block var="response"/></custom-block><l><option>list</option></l></block><script><block s="doReport"><block s="reportListItem"><l>2</l><custom-block s="$flash assoc %s %l"><l>text</l><block s="reportListItem"><l>2</l><custom-block s="$flash assoc %s %l"><l>data</l><block s="reportListItem"><l>1</l><block s="reportListItem"><l>2</l><custom-block s="$flash assoc %s %l"><l>completions</l><block var="response"/></custom-block></block></block></custom-block></block></custom-block></block><comment w="246.42857142857144" collapsed="true">Extracts the completion text from the response.</comment></block></script><script><block s="doReport"><block s="reportJoinWords"><list><l>There was an error: </l><block s="reportListItem"><l>2</l><block s="reportListItem"><l>1</l><block var="response"/></block></block></list></block><comment w="245.71428571428572" collapsed="true">Extracts the error message from the response.</comment></block></script></block></script></block-definition><block-definition s="maximum number of tokens %&apos;max&apos; (J1)" type="reporter" category="Jurassic 1"><comment x="0" y="0" w="255.00000000000017" collapsed="false">The maximum number of tokens to generate per result.&#xD;Optional, default = 16.&#xD;&#xD;If no stopSequences are given, generation is stopped&#xD;after producing maxTokens.</comment><header></header><code></code><translations></translations><inputs><input type="%n">16</input></inputs><script><block s="doReport"><block s="reportJoinWords"><list><l>"maxTokens": </l><block var="max"/></list></block></block></script></block-definition><block-definition s="temperature %&apos;temperature&apos; (J1)" type="reporter" category="Jurassic 1"><comment x="0" y="0" w="284.28571428571445" collapsed="false">Modifies the distribution from which tokens are sampled. Optional, default = 1.0.&#xD;&#xD;Setting temperature to 1.0 samples directly from the model distribution. Lower (higher) values increase the chance of sampling higher (lower) probability tokens. A value of 0 essentially disables sampling and results in greedy decoding, where the most likely token is chosen at every step.</comment><header></header><code></code><translations></translations><inputs><input type="%n"></input></inputs><script><block s="doReport"><block s="reportJoinWords"><list><l>"temperature": </l><block var="temperature"/></list></block></block></script></block-definition><block-definition s="top p %&apos;top p&apos; (J1)" type="reporter" category="Jurassic 1"><comment x="0" y="0" w="267.8571428571429" collapsed="false">Sample tokens from the corresponding top percentile of probability mass. Optional, default = 1.0.&#xD;&#xD;For example, a value of 0.9 will only consider tokens comprising the top 90% probability mass.</comment><header></header><code></code><translations></translations><inputs><input type="%n">1</input></inputs><script><block s="doReport"><block s="reportJoinWords"><list><l>"topP": </l><block var="top p"/></list></block></block></script></block-definition><block-definition s="presence penalty %&apos;penalty&apos; (J1)" type="reporter" category="Jurassic 1"><comment x="0" y="0" w="334.28571428571433" collapsed="false">Applies a fixed bias against generating tokens that appeared at least once in the prompt or in the completion. &#xD;&#xD;A positive penalty value implies reducing the probability of repetition.&#xD;Read more at&#xD;https://studio.ai21.com/docs/api/#repetition-penalties</comment><header></header><code></code><translations></translations><inputs><input type="%n">0</input></inputs><script><block s="doReport"><block s="reportJoinWords"><list><l>"presencePenalty": {"scale": </l><block var="penalty"/><l>}</l></list></block></block></script></block-definition><block-definition s="frequency penalty %&apos;penalty&apos; (J1)" type="reporter" category="Jurassic 1"><comment x="0" y="0" w="304.853515625" collapsed="false">Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&apos;s likelihood to repeat the same line verbatim.&#xD;&#xD;More info at&#xD;https://beta.openai.com/docs/api-reference/parameter-details</comment><header></header><code></code><translations></translations><inputs><input type="%n">0</input></inputs><script><block s="doReport"><block s="reportJoinWords"><list><l>"frequencyPenalty": {"scale": </l><block var="penalty"/><l>}</l></list></block></block></script></block-definition><block-definition s="stop completing before generating any of %&apos;stop&apos; (J1)" type="reporter" category="Jurassic 1"><comment x="0" y="0" w="269.2857142857143" collapsed="false">Stops decoding if any of the stop texts is generated.&#xD;&#xD;The decoded result text will not include the stop sequence text, but it will be included in the raw token data, which can also continue beyond the stop sequence if the sequence ended in the middle of a token. </comment><header></header><code></code><translations></translations><inputs><input type="%mult%txt"></input></inputs><script><block s="doIfElse"><block s="reportEquals"><block s="reportListAttribute"><l><option>length</option></l><block var="stop"/></block><l>1</l></block><script><block s="doReport"><block s="reportJoinWords"><list><l>"stopSequences": ["</l><block var="stop"/><l>"]</l></list></block></block></script><script><block s="doReport"><block s="reportJoinWords"><list><l>"stopSequences ": [</l><block s="reportMap"><block s="reifyReporter"><autolambda><block s="reportJoinWords"><list><l>"</l><l></l><l>",</l></list></block></autolambda><list></list></block><custom-block s="all but last %l"><block var="stop"/></custom-block></block><block s="reportJoinWords"><list><l>"</l><block s="reportListItem"><l><option>last</option></l><block var="stop"/></block><l>"</l></list></block><l>]</l></list></block></block></script></block></script></block-definition><block-definition s="count penalty %&apos;penalty&apos; (J1)" type="reporter" category="Jurassic 1"><comment x="0" y="0" w="304.853515625" collapsed="false">Applies a bias against generating tokens that appeared in the prompt or in the completion, proportional to the number of respective appearances. &#xD;&#xD;More info at&#xD;https://studio.ai21.com/docs/api/#repetition-penalties</comment><header></header><code></code><translations></translations><inputs><input type="%n">0</input></inputs><script><block s="doReport"><block s="reportJoinWords"><list><l>"countPenalty":  {"scale": </l><block var="penalty"/><l>}</l></list></block></block></script></block-definition><block-definition s="all but last %&apos;list&apos;" type="reporter" category="lists"><header></header><code></code><translations></translations><inputs><input type="%l"></input></inputs><script><block s="doIfElse"><block s="reportLessThanOrEquals"><block s="reportListAttribute"><l><option>length</option></l><block var="list"/></block><l>1</l></block><script><block s="doReport"><block s="reportNewList"><list></list></block></block></script><script><block s="doReport"><block s="reportListItem"><block s="reportNumbers"><l>1</l><block s="reportDifference"><block s="reportListAttribute"><l><option>length</option></l><block var="list"/></block><l>1</l></block></block><block var="list"/></block></block></script></block></script></block-definition><block-definition s="complete %&apos;prompt&apos; using GPT-3 engine named %&apos;engine&apos; with key %&apos;api key&apos; $nl with options %&apos;options&apos;" type="reporter" category="GPT-3 Blocks"><comment x="0" y="0" w="377.14285714285717" collapsed="false">Ask the specified GPT-3 engine to complete the prompt (or prompts).&#xD;Input 3 must be your OpenAI API key which you can obtain from https://openai.com&#xD;For documentation on the options see &#xD;https://beta.openai.com/docs/api-reference/completions/&#xD;</comment><header></header><code></code><translations></translations><inputs><input type="%txt"></input><input type="%txt">ada<options>ada&#xD;babbage&#xD;curie&#xD;davinci</options></input><input type="%txt"></input><input type="%mult%txt"></input></inputs><script><custom-block s="let %upvar be %s"><l>message</l><block s="reportJoinWords"><list><l>{</l><block s="reportMap"><block s="reifyReporter"><autolambda><block s="reportIfElse"><block s="reportEquals"><block s="reportStringSize"><l></l></block><l>0</l></block><l> </l><block s="reportJoinWords"><list><l></l><l>,</l></list></block><comment w="126.42857142857143" collapsed="true">Ignore empty options.</comment></block></autolambda><list></list></block><block var="options"/></block><custom-block s="prompt %txt"><block s="reportIfElse"><block s="reportIsA"><block var="prompt"/><l><option>text</option></l></block><custom-block s="sanitize prompt %txt"><block var="prompt"/></custom-block><block s="reportMap"><block s="reifyReporter"><autolambda><custom-block s="sanitize prompt %txt"><l></l></custom-block></autolambda><list></list></block><block var="prompt"/></block></block></custom-block><l>,</l><custom-block s="model %txt"><block s="reportIfElse"><block s="reportEquals"><block var="engine"/><l>davinci</l></block><l>text-davinci-002</l><block s="reportIfElse"><block s="reportListContainsItem"><block s="reportNewList"><list><l>ada</l><l>babbage</l><l>curie</l></list></block><block var="engine"/></block><block s="reportJoinWords"><list><l>text-</l><block var="engine"/><l>-001</l></list></block><block var="engine"/></block></block></custom-block><l>}</l></list></block></custom-block><custom-block s="let %upvar be %s"><l>response string</l><custom-block s="%s url: %s send: %s headers: %mult%l"><l>POST</l><l>https://api.openai.com/v1/completions</l><block var="message"/><list><custom-block s="key: %s value: %s"><l>Authorization</l><block s="reportJoinWords"><list><l>Bearer </l><block var="api key"/></list></block></custom-block><custom-block s="key: %s value: %s"><l>Content-Type</l><l>application/json</l></custom-block></list></custom-block></custom-block><block s="doIfElse"><block s="reportGreaterThan"><block s="reportStringSize"><block var="response string"/></block><l>0</l></block><script><custom-block s="let %upvar be %s"><l>response</l><block s="reportTextSplit"><block var="response string"/><l><option>json</option></l></block></custom-block><block s="doIfElse"><block s="reportIsA"><custom-block s="$flash assoc %s %l"><l>choices</l><block var="response"/></custom-block><l><option>list</option></l></block><script><custom-block s="let %upvar be %s"><l>choices</l><block s="reportListItem"><l>2</l><custom-block s="$flash assoc %s %l"><l>choices</l><block var="response"/></custom-block></block></custom-block><block s="doReport"><block s="reportIfElse"><block s="reportEquals"><block s="reportListAttribute"><l><option>length</option></l><block var="choices"/></block><l>1</l></block><block s="reportListItem"><l>2</l><custom-block s="$flash assoc %s %l"><l>text</l><block s="reportListItem"><l>1</l><block var="choices"/></block></custom-block></block><block s="reportMap"><block s="reifyReporter"><autolambda><block s="reportJoinWords"><list><block s="reportListItem"><l>2</l><custom-block s="$flash assoc %s %l"><l>text</l><l/></custom-block></block><custom-block s="multiline %mlt"><l>&#xD;</l></custom-block></list></block></autolambda><list></list></block><block var="choices"/></block></block><comment w="246.42857142857144" collapsed="true">Extracts the completion text from the response.</comment></block></script><script><block s="doReport"><block s="reportJoinWords"><list><l>There was an error: </l><block s="reportListItem"><l>2</l><custom-block s="$flash assoc %s %l"><l>message</l><block s="reportListItem"><l>2</l><custom-block s="$flash assoc %s %l"><l>error</l><block var="response"/></custom-block></block></custom-block></block><l> the message sent was </l><block var="message"/></list></block><comment w="245.71428571428572" collapsed="true">Extracts the error message from the response.</comment></block></script></block></script><script><block s="bubble"><l>No response from https://api.openai.com/</l></block></script></block></script><scripts><comment x="490.05509958791214" y="605.2142857142857" w="243.0460100446427" collapsed="false">Reports each completion in a list if more than one completion requested or prompts is a list</comment></scripts></block-definition><block-definition s="sanitize prompt %&apos;prompt&apos;" type="reporter" category="GPT-3 Blocks"><header></header><code></code><translations></translations><inputs><input type="%txt"></input></inputs><script><block s="doWarp"><script><block s="doReport"><custom-block s="remove trailing while space from %txt"><block s="reportJoinWords"><list><block s="reportMap"><block s="reifyReporter"><autolambda><block s="reportIfElse"><block s="reportEquals"><block s="reportUnicode"><l></l></block><l>39</l></block><block s="reportUnicodeAsLetter"><l>8217</l></block><block s="reportIfElse"><block s="reportEquals"><block s="reportUnicode"><l></l></block><l>34</l></block><block s="reportUnicodeAsLetter"><l>8220</l></block><block s="reportIfElse"><block s="reportEquals"><block s="reportUnicode"><l></l></block><l>10</l></block><l>\n</l><block s="reportIfElse"><block s="reportEquals"><block s="reportUnicode"><l></l></block><l>13</l></block><l> </l><l></l></block></block></block></block></autolambda><list></list></block><block s="reportTextSplit"><block var="prompt"/><l><option>letter</option></l></block><comment w="184.2857142857143" collapsed="false">Replaces &apos; with ’, &quot; with “, new lines with spaces, and removes trailing spaces.</comment></block></list></block></custom-block></block></script></block></script></block-definition><block-definition s="remove trailing while space from %&apos;text&apos;" type="reporter" category="variables"><header></header><code></code><translations></translations><inputs><input type="%txt"></input></inputs><script><custom-block s="let %upvar be %s"><l>letters</l><block s="reportTextSplit"><block var="text"/><l><option>letter</option></l></block></custom-block><block s="doUntil"><block s="reportNotEquals"><l> </l><block s="reportListItem"><l><option>last</option></l><block var="letters"/></block></block><script><block s="doSetVar"><l>letters</l><block s="reportListItem"><block s="reportNumbers"><l>1</l><block s="reportDifference"><block s="reportListAttribute"><l><option>length</option></l><block var="letters"/></block><l>1</l></block></block><block var="letters"/></block></block></script></block><block s="doReport"><block s="reportJoinWords"><list><block var="letters"/></list></block></block></script></block-definition><block-definition s="let %&apos;var&apos; be %&apos;value&apos;" type="command" category="other"><header></header><code></code><translations></translations><inputs><input type="%upvar"></input><input type="%s"></input></inputs><script><block s="doSetVar"><l>var</l><block var="value"/></block></script></block-definition><block-definition s="prompt %&apos;prompt&apos;" type="reporter" category="GPT-3 Blocks"><header></header><code></code><translations></translations><inputs><input type="%txt"></input></inputs><script><block s="doReport"><block s="reportJoinWords"><list><l>"prompt": </l><block s="reportIfElse"><block s="reportIsA"><block var="prompt"/><l><option>text</option></l></block><block s="reportJoinWords"><list><l>"</l><block var="prompt"/><l>"</l></list></block><block s="reportListAttribute"><l><option>json</option></l><block var="prompt"/></block></block></list></block></block></script></block-definition><block-definition s="model %&apos;name&apos;" type="reporter" category="GPT-3 Blocks"><header></header><code></code><translations></translations><inputs><input type="%txt"></input></inputs><script><block s="doReport"><block s="reportJoinWords"><list><l>"model": "</l><block var="name"/><l>"</l></list></block></block></script></block-definition><block-definition s="key: %&apos;key&apos; value: %&apos;value&apos;" type="reporter" category="operators"><header></header><code></code><translations>pt:um par (chave: _ , valor: _ )&#xD;</translations><inputs><input type="%s"></input><input type="%s"></input></inputs><script><block s="doReport"><block s="reportNewList"><list><block var="key"/><block var="value"/></list></block></block></script></block-definition><block-definition s="%&apos;method&apos; url: %&apos;url&apos; send: %&apos;payload&apos; headers: %&apos;headers&apos;" type="reporter" category="sensing"><header></header><code></code><translations>pt:a resposta a _ de _ enviando _ e cabeçalhos _&#xD;</translations><inputs><input type="%s" readonly="true">GET<options>GET&#xD;POST&#xD;PUT&#xD;DELETE</options></input><input type="%s">https://snap.berkeley.edu</input><input type="%s"></input><input type="%mult%l" readonly="true"></input></inputs><script><block s="doReport"><block s="reportApplyExtension"><l>xhr_request(mth, url, dta, hdrs)</l><list><block var="method"/><block var="url"/><block var="payload"/><block var="headers"/></list></block></block></script></block-definition><block-definition s="$flash assoc %&apos;key&apos; %&apos;a-list&apos;" type="reporter" category="lists"><comment x="0" y="0" w="289.3333333333333" collapsed="false">The second input is an &quot;association list,&quot; a list of two-item lists.  Each of those smaller lists has a &quot;key&quot; as its first item and a &quot;value&quot; as its second.  ASSOC reports the first key-value pair in the association list whose key matches the first input.</comment><header></header><code></code><translations>ca:associació _ _&#xD;</translations><inputs><input type="%s"></input><input type="%l"></input></inputs><script><block s="doReport"><block s="reportAtomicFindFirst"><block s="reifyPredicate"><autolambda><block s="reportEquals"><block var="key"/><block s="reportListItem"><l>1</l><l/></block></block></autolambda><list></list></block><block var="a-list"/></block></block></script></block-definition><block-definition s="multiline %&apos;text&apos;" type="reporter" category="operators"><header></header><code></code><translations>pt:o texto multilinha _&#xD;ca:multilínia _&#xD;</translations><inputs><input type="%mlt"></input></inputs><script><block s="doReport"><block var="text"/></block></script></block-definition><block-definition s="stop if generating %&apos;text&apos;" type="reporter" category="GPT-3 Blocks"><header></header><code></code><translations></translations><inputs><input type="%txt"></input></inputs><script><block s="doReport"><block s="reportJoinWords"><list><l>"stop": "</l><block var="text"/><l>"</l></list></block></block></script></block-definition><block-definition s="number of completions %&apos;n&apos; (GPT-3)" type="reporter" category="GPT-3 Blocks"><comment x="0" y="0" w="313.5714285714285" collapsed="false">How many completions to generate for each prompt.&#xD;&#xD;Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop.&#xD;&#xD;Details at&#xD;https://beta.openai.com/docs/api-reference/parameter-details</comment><header></header><code></code><translations></translations><inputs><input type="%n">2</input></inputs><script><block s="doReport"><block s="reportJoinWords"><list><l>"n": </l><block var="n"/></list></block></block></script></block-definition><block-definition s="best of %&apos;n&apos; (GPT-3)" type="reporter" category="GPT-3 Blocks"><comment x="0" y="0" w="313.5714285714285" collapsed="false">Generates best_of completions server-side and returns the &quot;best&quot; (the one with the highest log probability per token). Results cannot be streamed.&#xD;&#xD;When used with n, best_of controls the number of candidate completions and n specifies how many to return – best_of must be greater than n.&#xD;&#xD;Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop.&#xD;&#xD;Details at&#xD;https://beta.openai.com/docs/api-reference/parameter-details</comment><header></header><code></code><translations></translations><inputs><input type="%n">2</input></inputs><script><block s="doReport"><block s="reportJoinWords"><list><l>"best_of": </l><block var="n"/></list></block></block></script></block-definition></blocks><stage name="Stage" width="480" height="360" costume="0" color="255,255,255,1" tempo="60" threadsafe="false" penlog="false" volume="100" pan="0" lines="round" ternary="false" hyperops="true" codify="false" inheritance="true" sublistIDs="false" id="796"><pentrails>data:image/png;base64,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</pentrails><costumes><list struct="atomic" id="797"></list></costumes><sounds><list struct="atomic" id="798"></list></sounds><variables></variables><blocks></blocks><scripts></scripts><sprites select="1"><sprite name="Sprite" idx="1" x="0.4188481675391813" y="-0.7329842931937947" heading="90" scale="1" volume="100" pan="0" rotation="1" draggable="true" costume="0" color="80,80,80,1" pen="tip" id="803"><costumes><list struct="atomic" id="804"></list></costumes><sounds><list struct="atomic" id="805"></list></sounds><blocks></blocks><variables></variables><scripts><comment x="20.714285714285715" y="360.7142857142857" w="350.7142857142857" collapsed="true">AI21 Studio has a similar model to GPT-3. Compare and contrast.</comment><script x="37.142857142857146" y="250.68095238095242"><custom-block s="complete %txt using GPT-3 engine named %txt with key %txt %br with options %mult%txt"><block var="prompt"/><l>ada</l><block var="GPT-3 API key"/><list><custom-block s="temperature %n"><l>0.7</l></custom-block><custom-block s="maximum number of tokens %n"><l>100</l></custom-block><custom-block s="top p %n"><l>1</l></custom-block><custom-block s="presence penalty %n"><l>0</l></custom-block><custom-block s="frequency penalty %n"><l>0</l></custom-block><custom-block s="stop completing before generating any of %mult%txt"><list><l>Q:</l></list></custom-block></list></custom-block></script><script x="39.285714285714285" y="116.95238095238098"><block s="doSetVar"><l>prompt</l><custom-block s="multiline %mlt"><l>Q: Write a Python program that calculates the sum of all positive integers smaller than 8.&#xD;A: sum(x for x in range(8))&#xD;Q: Write a Python program that calculates the sum of squares of all positive integers between 2 and 13.&#xD;A:</l></custom-block></block></script><comment x="22.206333705357146" y="86.3809523809524" w="186.42857142857144" collapsed="true">Experiment with different prompts.</comment><comment x="19.90722656250008" y="220.10952380952384" w="247.85714285714286" collapsed="true">Experiment with different engines and options.</comment><script x="44.28571428571429" y="33.40476190476202"><block s="doAsk"><l>Enter your key into this transient variable</l><comment w="207.85714285714286" collapsed="true">You can get a key from openai.com</comment></block><block s="doSetVar"><l>GPT-3 API key</l><block s="getLastAnswer"></block></block></script><comment x="19.28571428571429" y="7.142857142857143" w="427.14285714285717" collapsed="true">First set your key to a transient variable that won&apos;t be saved if you save this project.</comment><script x="33.57142857142858" y="455.2857142857142"><custom-block s="complete %txt using Jurassic 1 engine named %txt with key %txt %br with options %mult%txt"><block var="prompt"/><l>j1-large</l><block var="Jurassic 1 API key"/><list><custom-block s="temperature %n (J1)"><l>0.7</l></custom-block><custom-block s="stop completing before generating any of %mult%txt (J1)"><list><l>Q:</l></list></custom-block><custom-block s="maximum number of tokens %n (J1)"><l>100</l></custom-block><custom-block s="top p %n (J1)"><l>1</l></custom-block><custom-block s="presence penalty %n (J1)"><l>0</l></custom-block><custom-block s="frequency penalty %n (J1)"><l>0</l></custom-block><custom-block s="count penalty %n (J1)"><l>0</l></custom-block></list></custom-block></script><script x="31.7142857142857" y="394.57142857142856"><block s="doAsk"><l>Enter your key into this transient variable</l><comment w="187.14285714285717" collapsed="true">You can get a key from ai21.com</comment></block><block s="doSetVar"><l>Jurassic 1 API key</l><block s="getLastAnswer"></block></block></script></scripts></sprite><watcher var="Jurassic 1 API key" style="normal" x="11.937499999999545" y="11.937500000000007" color="243,118,29" hidden="true"/><watcher var="prompt" style="normal" x="11.937499999999545" y="45.12375238750002" color="243,118,29"/></sprites></stage><variables><variable name="GPT-3 API key" transient="true"/><variable name="Jurassic 1 API key" transient="true"/><variable name="prompt"><l>Q: Write a Python program that calculates the sum of all positive integers smaller than 8.&#xD;A: sum(x for x in range(8))&#xD;Q: Write a Python program that calculates the sum of squares of all positive integers between 2 and 13.&#xD;A:</l></variable></variables></scene></scenes></project>